{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9aaf15",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "v19_part1_feature_engineering.py (Nihai Kapsamlı Mimari - Özellik Mühendisliği)\n",
    "\n",
    "Bu script, projenin tüm özellik mühendisliği adımlarını içerir:\n",
    "1.  Temel veri hazırlığı ve Özgün Etki Skorlarının hesaplanması.\n",
    "2.  Kullanıcı ve kategori için meta-modellerin eğitilmesi. (ÜRÜN MODELİ KALDIRILDI)\n",
    "3.  Tüm v7 özellikleri, v16 koşullu sızıntı özellikleri ve v18 meta-özelliklerini\n",
    "    birleştiren nihai, kapsamlı özellik setinin oluşturulması ve diske kaydedilmesi.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "from catboost import CatBoostRegressor\n",
    "import json\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ==============================================================================\n",
    "# --- BÖLÜM 1: TEMEL VERİ HAZIRLIĞI VE SKORLARIN HESAPLANMASI (v19) ---\n",
    "# ==============================================================================\n",
    "print(\"=\"*50)\n",
    "print(\"Bölüm 1: Temel Veri ve Skorların Hesaplanması (v19) Başladı.\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# --- Dosya Yolları ---\n",
    "BASE_DIR = './'\n",
    "os.makedirs(os.path.join(BASE_DIR, 'processed'), exist_ok=True)\n",
    "os.makedirs(os.path.join(BASE_DIR, 'models'), exist_ok=True)\n",
    "os.makedirs(os.path.join(BASE_DIR, 'submissions'), exist_ok=True)\n",
    "\n",
    "\n",
    "OUT_TRAIN_PATH = os.path.join(BASE_DIR, 'processed/train_processed_v19.csv')\n",
    "OUT_TEST_PATH = os.path.join(BASE_DIR, 'processed/test_processed_v19.csv')\n",
    "\n",
    "IN_TRAIN_PATH = os.path.join(BASE_DIR, 'train.csv')\n",
    "IN_TEST_PATH = os.path.join(BASE_DIR, 'test.csv')\n",
    "\n",
    "# --- Veri Yükleme ve Ön İşleme ---\n",
    "print(\"Ham veriler yükleniyor ve ön işleniyor...\")\n",
    "df_train_raw = pd.read_csv(IN_TRAIN_PATH, parse_dates=['event_time'])\n",
    "df_test_raw = pd.read_csv(IN_TEST_PATH, parse_dates=['event_time'])\n",
    "\n",
    "def fix_anomalous_sessions(df):\n",
    "    session_user_counts = df.groupby('user_session')['user_id'].nunique()\n",
    "    anomalous_sessions = session_user_counts[session_user_counts > 1].index\n",
    "    if len(anomalous_sessions) > 0:\n",
    "        anomalous_indices = df['user_session'].isin(anomalous_sessions)\n",
    "        df['user_session_original'] = df['user_session']\n",
    "        df.loc[anomalous_indices, 'user_session'] = df.loc[anomalous_indices, 'user_session'] + '_' + df.loc[anomalous_indices, 'user_id'].astype(str)\n",
    "    else:\n",
    "        df['user_session_original'] = df['user_session']\n",
    "    return df\n",
    "\n",
    "df_train_raw = fix_anomalous_sessions(df_train_raw)\n",
    "df_test_raw = fix_anomalous_sessions(df_test_raw)\n",
    "\n",
    "df_combined = pd.concat([df_train_raw.drop('session_value', axis=1), df_test_raw], ignore_index=True)\n",
    "\n",
    "# --- Ürün Filtreleme ---\n",
    "print(\"Ürün sayıları hesaplanıyor ve filtreleme yapılıyor...\")\n",
    "product_counts = df_train_raw['product_id'].value_counts()\n",
    "print(f\"Toplam ürün sayısı: {len(product_counts)}\")\n",
    "\n",
    "# 10'dan az kez geçen ürünleri bulma\n",
    "products_less_than_10 = product_counts[product_counts < 10]\n",
    "print(f\"10'dan az kez geçen ürün sayısı: {len(products_less_than_10)}\")\n",
    "\n",
    "# Bu ürünleri train verisinden çıkarma\n",
    "df_train_filtered = df_train_raw[~df_train_raw['product_id'].isin(products_less_than_10.index)].copy()\n",
    "print(f\"Filtreleme öncesi train boyutu: {len(df_train_raw)}\")\n",
    "print(f\"Filtreleme sonrası train boyutu: {len(df_train_filtered)}\")\n",
    "\n",
    "# Test verisini de aynı şekilde filtrele\n",
    "df_test_filtered = df_test_raw[~df_test_raw['product_id'].isin(products_less_than_10.index)].copy()\n",
    "print(f\"Filtreleme sonrası test boyutu: {len(df_test_filtered)}\")\n",
    "\n",
    "# Filtrelenmiş veriler ile devam et\n",
    "df_combined = pd.concat([df_train_filtered.drop('session_value', axis=1), df_test_filtered], ignore_index=True)\n",
    "\n",
    "# --- Özgün Etki Skorları için Baseline ve Delta Hesaplaması ---\n",
    "print(\"Özgün Etki Skorları için baseline'lar ve delta'lar hesaplanıyor...\")\n",
    "event_map = {'VIEW': 'V', 'ADD_CART': 'A', 'REMOVE_CART': 'R', 'BUY': 'B'}\n",
    "df_combined['event_short'] = df_combined['event_type'].map(event_map)\n",
    "signatures = df_combined.groupby('user_session')['event_short'].apply(lambda x: '_'.join(sorted(x)))\n",
    "df_train_filtered = df_train_filtered.merge(signatures.rename('behavioral_signature'), on='user_session', how='left')\n",
    "avg_value_for_signature = df_train_filtered.groupby('behavioral_signature')['session_value'].mean().to_dict()\n",
    "df_train_filtered['baseline_value'] = df_train_filtered['behavioral_signature'].map(avg_value_for_signature)\n",
    "df_train_filtered['delta'] = df_train_filtered['session_value'] - df_train_filtered['baseline_value']\n",
    "\n",
    "user_impact_scores = df_train_filtered.groupby('user_id')['delta'].mean().reset_index().rename(columns={'delta': 'actual_user_impact'})\n",
    "category_impact_scores = df_train_filtered.groupby('category_id')['delta'].mean().reset_index().rename(columns={'delta': 'actual_cat_impact'})\n",
    "\n",
    "# Ürün etki skoru hesaplaması geri eklendi\n",
    "product_impact_scores = df_train_filtered.groupby('product_id')['delta'].mean().reset_index().rename(columns={'delta': 'actual_prod_impact'})\n",
    "\n",
    "# ==============================================================================\n",
    "# --- BÖLÜM 2: META-MODELLERİN EĞİTİMİ (v19) ---\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Bölüm 2: Meta-Modellerin Eğitimi (v19) Başladı.\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# --- 2.1 Kullanıcı Meta-Modeli ---\n",
    "print(\"Kullanıcı meta-modeli eğitiliyor...\")\n",
    "user_agg = df_combined.groupby('user_id').agg(\n",
    "    user_event_count=('event_type', 'size'),\n",
    "    user_session_count=('user_session', 'nunique'),\n",
    "    user_nunique_products=('product_id', 'nunique'),\n",
    "    user_nunique_categories=('category_id', 'nunique')\n",
    ").reset_index()\n",
    "user_meta_data = user_agg.merge(user_impact_scores, on='user_id', how='left')\n",
    "\n",
    "user_meta_data_train = user_meta_data.dropna(subset=['actual_user_impact'])\n",
    "X_user_meta, y_user_meta = user_meta_data_train.drop(['user_id', 'actual_user_impact'], axis=1), user_meta_data_train['actual_user_impact']\n",
    "# DEĞİŞİKLİK: Parametreler artırıldı\n",
    "user_meta_model = CatBoostRegressor(iterations=3000, depth=7, verbose=0, random_seed=42)\n",
    "user_meta_model.fit(X_user_meta, y_user_meta)\n",
    "\n",
    "# DEĞİŞİKLİK: Ürün meta-modeli bölümü geri eklendi.\n",
    "# --- 2.2 Ürün Meta-Modeli ---\n",
    "print(\"Ürün meta-modeli eğitiliyor...\")\n",
    "prod_agg = pd.crosstab(df_combined['product_id'], df_combined['event_type'])\n",
    "prod_meta_data = prod_agg.merge(product_impact_scores, on='product_id', how='left')\n",
    "prod_meta_data_train = prod_meta_data.dropna(subset=['actual_prod_impact'])\n",
    "X_prod_meta, y_prod_meta = prod_meta_data_train.drop(['product_id', 'actual_prod_impact'], axis=1), prod_meta_data_train['actual_prod_impact']\n",
    "prod_meta_model = CatBoostRegressor(iterations=3000, depth=7, verbose=0, random_seed=42)\n",
    "prod_meta_model.fit(X_prod_meta, y_prod_meta)\n",
    "\n",
    "# --- 2.3 Kategori Meta-Modeli ---\n",
    "print(\"Kategori meta-modeli eğitiliyor...\")\n",
    "cat_agg = pd.crosstab(df_combined['category_id'], df_combined['event_type'])\n",
    "cat_meta_data = cat_agg.merge(category_impact_scores, on='category_id', how='left')\n",
    "cat_meta_data_train = cat_meta_data.dropna(subset=['actual_cat_impact'])\n",
    "X_cat_meta, y_cat_meta = cat_meta_data_train.drop(['category_id', 'actual_cat_impact'], axis=1), cat_meta_data_train['actual_cat_impact']\n",
    "# DEĞİŞİKLİK: Parametreler artırıldı\n",
    "cat_meta_model = CatBoostRegressor(iterations=3000, depth=7, verbose=0, random_seed=42)\n",
    "cat_meta_model.fit(X_cat_meta, y_cat_meta)\n",
    "\n",
    "# ==============================================================================\n",
    "# --- BÖLÜM 3: NİHAİ KAPSAMLI ÖZELLİK OLUŞTURMA (v19) ---\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Bölüm 3: Nihai Kapsamlı Özellik Oluşturma (v19) Başladı.\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# --- 3.1 Meta-Tahminleri ve Değer Katma Skorlarını Hesaplama ---\n",
    "print(\"Tüm elemanlar için meta-tahminler ve Değer Katma Skorları oluşturuluyor...\")\n",
    "all_users_agg = df_combined.groupby('user_id').agg(\n",
    "    user_event_count=('event_type', 'size'), user_session_count=('user_session', 'nunique'),\n",
    "    user_nunique_products=('product_id', 'nunique'), user_nunique_categories=('category_id', 'nunique')\n",
    ").reset_index()\n",
    "all_users_agg['predicted_user_impact'] = user_meta_model.predict(all_users_agg.drop('user_id', axis=1))\n",
    "all_users_agg = all_users_agg.merge(user_impact_scores, on='user_id', how='left')\n",
    "all_users_agg['user_value_add_score'] = all_users_agg['actual_user_impact'] - all_users_agg['predicted_user_impact']\n",
    "user_final_scores = all_users_agg[['user_id', 'predicted_user_impact', 'user_value_add_score']]\n",
    "\n",
    "# Ürün final skorları hesaplama bölümü geri eklendi.\n",
    "all_prods_agg = pd.crosstab(df_combined['product_id'], df_combined['event_type'])\n",
    "all_prods_agg['predicted_prod_impact'] = prod_meta_model.predict(all_prods_agg)\n",
    "all_prods_agg = all_prods_agg.reset_index().merge(product_impact_scores, on='product_id', how='left')\n",
    "all_prods_agg['prod_value_add_score'] = all_prods_agg['actual_prod_impact'] - all_prods_agg['predicted_prod_impact']\n",
    "prod_final_scores = all_prods_agg[['product_id', 'predicted_prod_impact', 'prod_value_add_score']]\n",
    "\n",
    "all_cats_agg = pd.crosstab(df_combined['category_id'], df_combined['event_type'])\n",
    "all_cats_agg['predicted_cat_impact'] = cat_meta_model.predict(all_cats_agg)\n",
    "all_cats_agg = all_cats_agg.reset_index().merge(category_impact_scores, on='category_id', how='left')\n",
    "all_cats_agg['cat_value_add_score'] = all_cats_agg['actual_cat_impact'] - all_cats_agg['predicted_cat_impact']\n",
    "cat_final_scores = all_cats_agg[['category_id', 'predicted_cat_impact', 'cat_value_add_score']]\n",
    "\n",
    "# --- 3.2 v7'nin Tüm Kullanıcı Özelliklerini Hesaplama ---\n",
    "print(\"v7'nin tüm kullanıcı seviyesi özellikleri hesaplanıyor...\")\n",
    "user_v7_features = df_combined.groupby('user_id').agg(\n",
    "    user_total_events=('event_type', 'count'),\n",
    "    user_unique_products_viewed=('product_id', 'nunique'),\n",
    "    user_first_seen=('event_time', 'min'),\n",
    "    user_last_seen=('event_time', 'max')\n",
    ")\n",
    "user_v7_features['user_lifespan_days'] = (user_v7_features['user_last_seen'] - user_v7_features['user_first_seen']).dt.days\n",
    "user_buy_counts = df_combined[df_combined['event_type'] == 'BUY'].groupby('user_id').size().rename('user_buy_count')\n",
    "user_v7_features = user_v7_features.join(user_buy_counts)\n",
    "user_v7_features['user_purchase_rate'] = user_v7_features['user_buy_count'] / user_v7_features['user_total_events']\n",
    "user_v7_features.drop(['user_first_seen', 'user_last_seen'], axis=1, inplace=True)\n",
    "\n",
    "# --- 3.3 Ana Fonksiyon: v7 + v16 + v18 Sentezi ---\n",
    "def create_final_features_v19(df, is_train=True):\n",
    "    print(f\"\\n{'Train' if is_train else 'Test'} seti için v19 Sentez özellikleri oluşturuluyor...\")\n",
    "    df = df.sort_values(by=['user_session', 'event_time'])\n",
    "    \n",
    "    df['time_diff'] = df.groupby('user_session')['event_time'].diff().dt.total_seconds()\n",
    "    df['event_order'] = df.groupby('user_session').cumcount() + 1\n",
    "    \n",
    "    agg_dict = {\n",
    "        'user_id': 'first', 'event_type': 'count', 'product_id': ['nunique', list],\n",
    "        'category_id': ['nunique', list], 'event_time': ['min', 'max', lambda x: x.dt.hour.mean(), lambda x: x.dt.dayofweek.mean()],\n",
    "        'time_diff': ['mean', 'max', 'std', 'median'], 'event_order': 'mean'\n",
    "    }\n",
    "    if is_train: agg_dict['session_value'] = 'first'\n",
    "    \n",
    "    sf = df.groupby('user_session').agg(agg_dict)\n",
    "    sf['user_session_original'] = df.groupby('user_session')['user_session_original'].first()\n",
    "\n",
    "    col_names = [\n",
    "        'user_id', 'event_count', 'unique_products', 'products', 'unique_categories', 'categories',\n",
    "        'start_time', 'end_time', 'avg_hour', 'avg_dayofweek', 'avg_time_diff', 'max_time_diff',\n",
    "        'std_time_diff', 'median_time_diff', 'avg_event_order',\n",
    "    ]\n",
    "    if is_train:\n",
    "        col_names.append('session_value')\n",
    "    \n",
    "    col_names.append('user_session_original')\n",
    "\n",
    "    sf.columns = col_names\n",
    "    \n",
    "    sf['duration'] = (sf['end_time'] - sf['start_time']).dt.total_seconds()\n",
    "    sf['avg_event_order_pct'] = sf['avg_event_order'] / sf['event_count']\n",
    "    \n",
    "    event_counts = pd.crosstab(df['user_session'], df['event_type'])\n",
    "    sf = sf.join(event_counts)\n",
    "    for event in ['VIEW', 'ADD_CART', 'REMOVE_CART', 'BUY']:\n",
    "        if event not in sf.columns: sf[event] = 0\n",
    "    \n",
    "    sf['net_cart_additions'] = sf['ADD_CART'] - sf['REMOVE_CART']\n",
    "    sf['view_to_add_cart_rate'] = sf['ADD_CART'] / (sf['VIEW'] + 1e-6)\n",
    "    sf['add_cart_to_buy_rate'] = sf['BUY'] / (sf['ADD_CART'] + 1e-6)\n",
    "    sf['view_to_buy_rate'] = sf['BUY'] / (sf['VIEW'] + 1e-6)\n",
    "    sf['did_purchase'] = (sf['BUY'] > 0).astype(int)\n",
    "\n",
    "    sf = sf.merge(user_v7_features, on='user_id', how='left')\n",
    "    sf = sf.merge(user_final_scores, on='user_id', how='left')\n",
    "    \n",
    "    # Ürün meta özelliklerini ekleyen bölüm geri eklendi.\n",
    "    prod_scores_map = prod_final_scores.set_index('product_id')\n",
    "    def get_prod_meta_scores(products):\n",
    "        if not products:\n",
    "            return pd.Series([0,0], index=['mean_pred_prod_impact', 'mean_prod_value_add'])\n",
    "        scores = prod_scores_map.reindex(list(set(products)))\n",
    "        if scores.empty:\n",
    "            return pd.Series([0,0], index=['mean_pred_prod_impact', 'mean_prod_value_add'])\n",
    "        mean_scores = scores[['predicted_prod_impact', 'prod_value_add_score']].mean()\n",
    "        mean_scores = mean_scores.rename(index={'predicted_prod_impact': 'mean_pred_prod_impact', 'prod_value_add_score': 'mean_prod_value_add'})\n",
    "        return mean_scores\n",
    "        \n",
    "    meta_prod_df = sf['products'].apply(get_prod_meta_scores)\n",
    "    sf = pd.concat([sf, meta_prod_df], axis=1)\n",
    "\n",
    "    cat_scores_map = cat_final_scores.set_index('category_id')\n",
    "    def get_cat_meta_scores(categories):\n",
    "        if not categories:\n",
    "            return pd.Series([0,0], index=['mean_pred_cat_impact', 'mean_cat_value_add'])\n",
    "        scores = cat_scores_map.reindex(list(set(categories)))\n",
    "        if scores.empty:\n",
    "            return pd.Series([0,0], index=['mean_pred_cat_impact', 'mean_cat_value_add'])\n",
    "        mean_scores = scores[['predicted_cat_impact', 'cat_value_add_score']].mean()\n",
    "        mean_scores = mean_scores.rename(index={'predicted_cat_impact': 'mean_pred_cat_impact', 'cat_value_add_score': 'mean_cat_value_add'})\n",
    "        return mean_scores\n",
    "        \n",
    "    meta_cat_df = sf['categories'].apply(get_cat_meta_scores)\n",
    "    sf = pd.concat([sf, meta_cat_df], axis=1)\n",
    "\n",
    "    sf = sf.set_index('user_session_original', drop=True)\n",
    "    \n",
    "    sf.drop(['user_id', 'products', 'categories', 'start_time', 'end_time'], axis=1, inplace=True)\n",
    "    return sf\n",
    "\n",
    "df_session_train = create_final_features_v19(df_train_filtered, is_train=True)\n",
    "df_session_test = create_final_features_v19(df_test_filtered, is_train=False)\n",
    "\n",
    "train_cols = df_session_train.columns.drop('session_value')\n",
    "df_session_test = df_session_test.reindex(columns=train_cols)\n",
    "df_session_test.fillna(0, inplace=True)\n",
    "\n",
    "df_session_train.to_csv(OUT_TRAIN_PATH, index=True)\n",
    "df_session_test.to_csv(OUT_TEST_PATH, index=True)\n",
    "print(f\"\\nİşlenmiş veriler kaydedildi. Train shape: {df_session_train.shape}, Test shape: {df_session_test.shape}\")\n",
    "print(\"\\nBölüm 3 Tamamlandı.\")\n",
    "gc.collect()\n",
    "\n",
    "# ==============================================================================\n",
    "# --- BÖLÜM 4: NİHAİ ANA MODEL EĞİTİMİ (v19) ---\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Bölüm 4: Nihai Ana Model Eğitimi (v19) Başladı.\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# --- Dosya Yolları ---\n",
    "BASE_DIR = './'\n",
    "IN_TRAIN_PATH = os.path.join(BASE_DIR, 'processed/train_processed_v19.csv')\n",
    "IN_TEST_PATH = os.path.join(BASE_DIR, 'processed/test_processed_v19.csv')\n",
    "OUT_MODEL_PATH = os.path.join(BASE_DIR, 'models/catboost_model_v19.cbm')\n",
    "OUT_FEATURES_PATH = os.path.join(BASE_DIR, 'models/features_v19.json')\n",
    "\n",
    "# --- İşlenmiş Veriyi Yükleme ---\n",
    "try:\n",
    "    df_train = pd.read_csv(IN_TRAIN_PATH, index_col=0)\n",
    "except FileNotFoundError:\n",
    "    print(f\"HATA: '{IN_TRAIN_PATH}' dosyası bulunamadı. Lütfen ilk script'i çalıştırdığınızdan emin olun.\")\n",
    "    exit()\n",
    "    \n",
    "df_train.dropna(subset=['session_value'], inplace=True)\n",
    "y = df_train['session_value']\n",
    "X = df_train.drop('session_value', axis=1)\n",
    "features = X.columns.tolist()\n",
    "y_log = np.log1p(y)\n",
    "\n",
    "# 80-20 train-test bölme\n",
    "X_train, X_val, y_train_log, y_val_log = train_test_split(X, y_log, test_size=0.2, shuffle=True, random_state=42)\n",
    "# Orijinal değerler de ayrı tutulacak MSE hesabı için\n",
    "y_train_orig, y_val_orig = train_test_split(y, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"Nihai özellik sayısı: {len(features)}\")\n",
    "print(f\"Eğitim seti boyutu: {X_train.shape[0]}, Doğrulama seti boyutu: {X_val.shape[0]}\")\n",
    "\n",
    "print(\"\\nCatBoost Ana Modeli eğitimi başlıyor...\")\n",
    "# DEĞİŞİKLİK: Ana modelin parametreleri artırıldı.\n",
    "model = CatBoostRegressor(\n",
    "    iterations=10000,\n",
    "    learning_rate=0.015,\n",
    "    depth=8,\n",
    "    l2_leaf_reg=5,\n",
    "    loss_function='Tweedie:variance_power=1.8',\n",
    "    eval_metric='RMSE',\n",
    "    random_seed=42,\n",
    "    verbose=500,\n",
    "    early_stopping_rounds=400\n",
    ")\n",
    "model.fit(X_train, y_train_log, eval_set=(X_val, y_val_log))\n",
    "\n",
    "val_preds = np.expm1(model.predict(X_val))\n",
    "rmse = np.sqrt(mean_squared_error(np.expm1(y_val_log), val_preds))\n",
    "mse = mean_squared_error(y_val_orig, val_preds)\n",
    "print(f\"\\nValidation RMSE: {rmse:.4f}\")\n",
    "print(f\"Validation MSE: {mse:.4f}\")\n",
    "\n",
    "print(\"\\nTüm veri üzerinde nihai ana model eğitiliyor...\")\n",
    "final_model = CatBoostRegressor(**model.get_params())\n",
    "final_model.set_params(iterations=model.get_best_iteration())\n",
    "final_model.fit(X, y_log, verbose=False)\n",
    "\n",
    "final_model.save_model(OUT_MODEL_PATH)\n",
    "with open(OUT_FEATURES_PATH, 'w') as f:\n",
    "    json.dump(features, f)\n",
    "\n",
    "print(f\"Model '{OUT_MODEL_PATH}' dosyasına kaydedildi.\")\n",
    "print(f\"Özellik listesi '{OUT_FEATURES_PATH}' dosyasına kaydedildi.\")\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# --- BÖLÜM 5: SUBMISSION OLUŞTURMA (v19 - NİHAİ DÜZELTİLMİŞ) ---\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Bölüm 5: Submission Oluşturma (v19) Başladı.\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# --- Gerekli Dosyaları Yükleme ---\n",
    "IN_SUBMISSION_PATH = os.path.join(BASE_DIR, 'sample_submission.csv')\n",
    "RAW_TRAIN_PATH = os.path.join(BASE_DIR, 'train.csv')\n",
    "RAW_TEST_PATH = os.path.join(BASE_DIR, 'test.csv')\n",
    "OUT_SUBMISSION_PATH = os.path.join(BASE_DIR, 'submissions/submission_v19.csv')\n",
    "\n",
    "df_test_processed = pd.read_csv(IN_TEST_PATH, index_col=0)\n",
    "df_submission_template = pd.read_csv(IN_SUBMISSION_PATH)\n",
    "df_test_raw_leak = pd.read_csv(RAW_TEST_PATH)\n",
    "df_test_raw_leak = fix_anomalous_sessions(df_test_raw_leak) # Ham test setine de original_session'ı ekle\n",
    "\n",
    "# Modelin beklediği sırayı garantile\n",
    "df_test_processed = df_test_processed[features]\n",
    "\n",
    "# --- Tahminleri Yapma ---\n",
    "print(\"Test seti üzerinde tahminler yapılıyor...\")\n",
    "predictions = np.expm1(final_model.predict(df_test_processed))\n",
    "predictions[predictions < 0] = 0\n",
    "\n",
    "# Tahminleri, İŞLENMİŞ (düzeltilmiş) session ID'leri ile bir DataFrame'e koy\n",
    "df_predictions = pd.DataFrame({'user_session': df_test_processed.index, 'predicted_value': predictions})\n",
    "\n",
    "# Orijinal session ID'lerini tahminlere ekle\n",
    "session_map = df_test_raw_leak[['user_session', 'user_session_original']].drop_duplicates().set_index('user_session')\n",
    "df_predictions = df_predictions.merge(session_map, on='user_session', how='left')\n",
    "\n",
    "\n",
    "# --- Adım 1: Normal Tahminleri Eşleme ---\n",
    "submission_map = df_predictions.set_index('user_session_original')['predicted_value'].to_dict()\n",
    "df_submission = df_submission_template.copy()\n",
    "df_submission['session_value'] = df_submission['user_session'].map(submission_map)\n",
    "\n",
    "\n",
    "# --- Adım 2: Anormal Seanslar için Akıllı Doldurma ---\n",
    "session_user_counts = pd.read_csv(RAW_TEST_PATH).groupby('user_session')['user_id'].nunique()\n",
    "anormal_sessions_orig = session_user_counts[session_user_counts > 1].index.tolist()\n",
    "print(f\"\\n{len(anormal_sessions_orig)} adet anormal seans için akıllı toplama işlemi yapılıyor...\")\n",
    "\n",
    "for session_id in anormal_sessions_orig:\n",
    "    constituent_preds = df_predictions[df_predictions['user_session_original'] == session_id]\n",
    "    \n",
    "    if not constituent_preds.empty:\n",
    "        total_value = constituent_preds['predicted_value'].sum()\n",
    "        df_submission.loc[df_submission['user_session'] == session_id, 'session_value'] = total_value\n",
    "        print(f\"'{session_id}' için {len(constituent_preds)} parçanın tahmini toplandı: {total_value:.4f}\")\n",
    "\n",
    "# --- Adım 3: Nihai Adım: Doğrulanmış Sızıntıyı Ezme ---\n",
    "print(\"\\nDoğrulanmış sızıntı (verified leaks) uygulanıyor...\")\n",
    "df_train_leak = df_train_filtered  # Filtrelenmiş train verisini kullan\n",
    "train_session_users = df_train_leak.groupby('user_session_original')['user_id'].apply(set)\n",
    "test_session_users = df_test_filtered.groupby('user_session_original')['user_id'].apply(set)\n",
    "\n",
    "common_sessions_final = set(train_session_users.index).intersection(set(test_session_users.index))\n",
    "verified_leaked_sessions = {sid for sid in common_sessions_final if train_session_users.get(sid) == test_session_users.get(sid)}\n",
    "leak_map = df_train_leak[df_train_leak['user_session_original'].isin(verified_leaked_sessions)].groupby('user_session_original')['session_value'].first().to_dict()\n",
    "print(f\"{len(leak_map)} adet tahmin, doğrulanmış gerçek değerlerle eziliyor...\")\n",
    "\n",
    "# update metodu için index ayarlama\n",
    "df_submission = df_submission.set_index('user_session')\n",
    "df_submission['session_value'].update(pd.Series(leak_map))\n",
    "df_submission.reset_index(inplace=True)\n",
    "\n",
    "expected_rows = 30789\n",
    "print(f\"\\nNihai dosyadaki satır sayısı: {len(df_submission)}. Beklenen: {expected_rows}\")\n",
    "if len(df_submission) != expected_rows:\n",
    "    print(f\"UYARI: Satır sayısı eşleşmiyor! Lütfen kontrol edin.\")\n",
    "else:\n",
    "    print(\"Satır sayısı DOĞRU.\")\n",
    "\n",
    "# Olası NaN değerleri 0 ile doldur\n",
    "df_submission['session_value'].fillna(0, inplace=True)\n",
    "\n",
    "df_submission.to_csv(OUT_SUBMISSION_PATH, index=False)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"İŞLEM TAMAMLANDI! Nihai submission dosyası '{OUT_SUBMISSION_PATH}' oluşturuldu.\")\n",
    "print(\"=\"*50)\n",
    "print(\"Nihai Dosyanın Başı:\")\n",
    "print(df_submission.head())\n",
    "\n",
    "# ==============================================================================\n",
    "# --- BÖLÜM 6: ID ETKİLERİ ANALİZİ ---\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BÖLÜM 6: ÜRÜN ID, KATEGORİ ID VE KULLANICI ID ETKİLERİ ANALİZİ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Analiz sonuçlarını saklamak için liste\n",
    "analysis_results = []\n",
    "\n",
    "# İşlenmiş train verisini yükle\n",
    "df_train_analysis = pd.read_csv(IN_TRAIN_PATH, index_col=0)\n",
    "df_train_analysis.dropna(subset=['session_value'], inplace=True)\n",
    "y_analysis = df_train_analysis['session_value']\n",
    "y_log_analysis = np.log1p(y_analysis)\n",
    "\n",
    "# ID ile ilgili özellikleri belirle\n",
    "user_features = [col for col in df_train_analysis.columns if 'user' in col.lower()]\n",
    "product_features = [col for col in df_train_analysis.columns if 'prod' in col.lower()]\n",
    "category_features = [col for col in df_train_analysis.columns if 'cat' in col.lower()]\n",
    "\n",
    "# Çakışan özellikleri temizle\n",
    "user_specific_features = [col for col in user_features if 'prod' not in col.lower() and 'cat' not in col.lower()]\n",
    "product_specific_features = [col for col in product_features if 'user' not in col.lower() and 'cat' not in col.lower()]\n",
    "category_specific_features = [col for col in category_features if 'user' not in col.lower() and 'prod' not in col.lower()]\n",
    "\n",
    "# Genel özellikler (multiple ID türlerinde kullanılanlar)\n",
    "general_features = ['unique_products', 'unique_categories']  # Bu özellikler hem ürün hem kategori ile ilgili\n",
    "\n",
    "# Tüm ID özelliklerini birleştir (tekrar olmayacak şekilde)\n",
    "all_id_features = list(set(user_specific_features + product_specific_features + category_specific_features + general_features))\n",
    "\n",
    "print(f\"Kullanıcı özellikleri ({len(user_specific_features)}): {user_specific_features}\")\n",
    "print(f\"Ürün özellikleri ({len(product_specific_features)}): {product_specific_features}\")  \n",
    "print(f\"Kategori özellikleri ({len(category_specific_features)}): {category_specific_features}\")\n",
    "print(f\"Genel özellikler ({len(general_features)}): {general_features}\")\n",
    "\n",
    "# Baseline özellikler (ID'ler hariç)\n",
    "baseline_features = [col for col in df_train_analysis.columns if col not in all_id_features and col != 'session_value']\n",
    "print(f\"Baseline özellikler ({len(baseline_features)}): {baseline_features[:5]}...\")\n",
    "\n",
    "def train_and_evaluate_model(X_features, feature_name, iteration_count=2000):\n",
    "    \"\"\"Model eğitip MSE hesaplayan fonksiyon\"\"\"\n",
    "    print(f\"\\n{feature_name} ile model eğitiliyor...\")\n",
    "    \n",
    "    X_temp = df_train_analysis[X_features]\n",
    "    X_train_temp, X_val_temp, y_train_temp, y_val_temp = train_test_split(\n",
    "        X_temp, y_log_analysis, test_size=0.2, random_state=42\n",
    "    )\n",
    "    y_train_orig_temp, y_val_orig_temp = train_test_split(\n",
    "        y_analysis, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    model_temp = CatBoostRegressor(\n",
    "        iterations=iteration_count,\n",
    "        learning_rate=0.02,\n",
    "        depth=6,\n",
    "        verbose=0,\n",
    "        random_seed=42\n",
    "    )\n",
    "    model_temp.fit(X_train_temp, y_train_temp)\n",
    "    \n",
    "    val_preds_temp = np.expm1(model_temp.predict(X_val_temp))\n",
    "    mse_temp = mean_squared_error(y_val_orig_temp, val_preds_temp)\n",
    "    rmse_temp = np.sqrt(mse_temp)\n",
    "    \n",
    "    print(f\"{feature_name} - MSE: {mse_temp:.4f}, RMSE: {rmse_temp:.4f}\")\n",
    "    return mse_temp, rmse_temp\n",
    "\n",
    "# 1. Baseline Model (ID'ler olmadan)\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"1. BASELINE MODEL (ID ÖZELLİKLERİ OLMADAN)\")\n",
    "print(\"-\"*60)\n",
    "baseline_mse, baseline_rmse = train_and_evaluate_model(baseline_features, \"Baseline\")\n",
    "analysis_results.append({\n",
    "    'Model': 'Baseline (ID\\'ler olmadan)',\n",
    "    'Özellik Sayısı': len(baseline_features),\n",
    "    'MSE': baseline_mse,\n",
    "    'RMSE': baseline_rmse,\n",
    "    'MSE İyileştirme': 0,\n",
    "    'MSE İyileştirme %': 0\n",
    "})\n",
    "\n",
    "# 2. Baseline + Sadece Ürün ID\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"2. BASELINE + SADECE ÜRÜN ID ÖZELLİKLERİ\")\n",
    "print(\"-\"*60)\n",
    "# unique_products'ı ürün özelliği olarak kabul et, tekrar ekleme\n",
    "product_features_combined = baseline_features + product_specific_features\n",
    "# unique_products zaten product_specific_features'da değilse ekle\n",
    "if 'unique_products' not in product_features_combined:\n",
    "    product_features_combined.append('unique_products')\n",
    "product_mse, product_rmse = train_and_evaluate_model(product_features_combined, \"Baseline + Ürün ID\")\n",
    "product_improvement = baseline_mse - product_mse\n",
    "product_improvement_pct = (product_improvement / baseline_mse) * 100\n",
    "analysis_results.append({\n",
    "    'Model': 'Baseline + Ürün ID',\n",
    "    'Özellik Sayısı': len(product_features_combined),\n",
    "    'MSE': product_mse,\n",
    "    'RMSE': product_rmse,\n",
    "    'MSE İyileştirme': product_improvement,\n",
    "    'MSE İyileştirme %': product_improvement_pct\n",
    "})\n",
    "\n",
    "# 3. Baseline + Sadece Kullanıcı ID\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"3. BASELINE + SADECE KULLANICI ID ÖZELLİKLERİ\")\n",
    "print(\"-\"*60)\n",
    "user_features_combined = baseline_features + user_specific_features\n",
    "user_mse, user_rmse = train_and_evaluate_model(user_features_combined, \"Baseline + Kullanıcı ID\")\n",
    "user_improvement = baseline_mse - user_mse\n",
    "user_improvement_pct = (user_improvement / baseline_mse) * 100\n",
    "analysis_results.append({\n",
    "    'Model': 'Baseline + Kullanıcı ID',\n",
    "    'Özellik Sayısı': len(user_features_combined),\n",
    "    'MSE': user_mse,\n",
    "    'RMSE': user_rmse,\n",
    "    'MSE İyileştirme': user_improvement,\n",
    "    'MSE İyileştirme %': user_improvement_pct\n",
    "})\n",
    "\n",
    "# 4. Baseline + Sadece Kategori ID\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"4. BASELINE + SADECE KATEGORİ ID ÖZELLİKLERİ\")\n",
    "print(\"-\"*60)\n",
    "category_features_combined = baseline_features + category_specific_features\n",
    "# unique_categories'ı kategori özelliği olarak kabul et, tekrar ekleme\n",
    "if 'unique_categories' not in category_features_combined:\n",
    "    category_features_combined.append('unique_categories')\n",
    "category_mse, category_rmse = train_and_evaluate_model(category_features_combined, \"Baseline + Kategori ID\")\n",
    "category_improvement = baseline_mse - category_mse\n",
    "category_improvement_pct = (category_improvement / baseline_mse) * 100\n",
    "analysis_results.append({\n",
    "    'Model': 'Baseline + Kategori ID',\n",
    "    'Özellik Sayısı': len(category_features_combined),\n",
    "    'MSE': category_mse,\n",
    "    'RMSE': category_rmse,\n",
    "    'MSE İyileştirme': category_improvement,\n",
    "    'MSE İyileştirme %': category_improvement_pct\n",
    "})\n",
    "\n",
    "# 5. Tüm özellikler birlikte\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"5. TÜM ÖZELLİKLER BİRLİKTE (BASELINE + TÜM ID'LER)\")\n",
    "print(\"-\"*60)\n",
    "# Tüm benzersiz özellikleri birleştir\n",
    "all_features_combined = list(set(baseline_features + user_specific_features + product_specific_features + category_specific_features + ['unique_products', 'unique_categories']))\n",
    "all_mse, all_rmse = train_and_evaluate_model(all_features_combined, \"Tüm Özellikler\")\n",
    "all_improvement = baseline_mse - all_mse\n",
    "all_improvement_pct = (all_improvement / baseline_mse) * 100\n",
    "analysis_results.append({\n",
    "    'Model': 'Tüm Özellikler Birlikte',\n",
    "    'Özellik Sayısı': len(all_features_combined),\n",
    "    'MSE': all_mse,\n",
    "    'RMSE': all_rmse,\n",
    "    'MSE İyileştirme': all_improvement,\n",
    "    'MSE İyileştirme %': all_improvement_pct\n",
    "})\n",
    "\n",
    "# Analiz sonuçları tablosu\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ID ETKİLERİ ANALİZ TABLOSU\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "results_df = pd.DataFrame(analysis_results)\n",
    "print(results_df.to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "# En iyi performansları göster\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ÖNEMLI BULGULAR\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "best_individual_model = results_df.iloc[1:4].loc[results_df.iloc[1:4]['MSE İyileştirme %'].idxmax()]\n",
    "print(f\"• En iyi tekil ID etkisi: {best_individual_model['Model']} - %{best_individual_model['MSE İyileştirme %']:.2f} iyileştirme\")\n",
    "\n",
    "worst_individual_model = results_df.iloc[1:4].loc[results_df.iloc[1:4]['MSE İyileştirme %'].idxmin()]\n",
    "print(f\"• En zayıf tekil ID etkisi: {worst_individual_model['Model']} - %{worst_individual_model['MSE İyileştirme %']:.2f} iyileştirme\")\n",
    "\n",
    "total_improvement = results_df.iloc[-1]['MSE İyileştirme %']\n",
    "sum_individual_improvements = results_df.iloc[1:4]['MSE İyileştirme %'].sum()\n",
    "synergy_effect = total_improvement - sum_individual_improvements\n",
    "\n",
    "print(f\"• Bireysel ID etkilerinin toplamı: %{sum_individual_improvements:.2f}\")\n",
    "print(f\"• Tüm ID'lerin birlikte etkisi: %{total_improvement:.2f}\")\n",
    "print(f\"• Sinerji etkisi: %{synergy_effect:.2f}\")\n",
    "\n",
    "if synergy_effect > 0:\n",
    "    print(\"  → ID'ler birlikte kullanıldığında POZİTİF sinerji var!\")\n",
    "else:\n",
    "    print(\"  → ID'ler birlikte kullanıldığında NEGATİF etkileşim var!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
