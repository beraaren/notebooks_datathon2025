{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhNiQJrh_7G4"
      },
      "source": [
        "# V7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nEjuUO4l7IH",
        "outputId": "dda6b9cf-a677-46f4-9d96-1c2e9ee81671"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting catboost\n",
            "  Using cached catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting graphviz (from catboost)\n",
            "  Using cached graphviz-0.21-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: matplotlib in ./.venv/lib/python3.12/site-packages (from catboost) (3.10.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in ./.venv/lib/python3.12/site-packages (from catboost) (2.3.2)\n",
            "Requirement already satisfied: pandas>=0.24 in ./.venv/lib/python3.12/site-packages (from catboost) (2.3.2)\n",
            "Requirement already satisfied: scipy in ./.venv/lib/python3.12/site-packages (from catboost) (1.16.1)\n",
            "Requirement already satisfied: plotly in ./.venv/lib/python3.12/site-packages (from catboost) (6.3.0)\n",
            "Requirement already satisfied: six in ./.venv/lib/python3.12/site-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib->catboost) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.12/site-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: narwhals>=1.15.1 in ./.venv/lib/python3.12/site-packages (from plotly->catboost) (2.2.0)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading graphviz-0.21-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: graphviz, catboost\n",
            "Successfully installed catboost-1.2.8 graphviz-0.21\n"
          ]
        }
      ],
      "source": [
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ortalama: 41.438740199599046\n",
            "Standart Sapma: 44.11533396262052\n",
            "Maksimum: 713.970248534592\n",
            "Minimum: 9.140855995108964\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_submission = pd.read_csv('./submission.csv')\n",
        "session_values = df_submission['session_value']\n",
        "\n",
        "print(\"Ortalama:\", session_values.mean())\n",
        "print(\"Standart Sapma:\", session_values.std())\n",
        "print(\"Maksimum:\", session_values.max())\n",
        "print(\"Minimum:\", session_values.min())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "V8_Pseudo_Labeling_Integrated_Script.py\n",
        "\n",
        "This script combines data preparation, a teacher-student pseudo-labeling framework,\n",
        "hyperparameter optimization for the pseudo-labeling process, and submission generation\n",
        "into a single, cohesive workflow.\n",
        "\n",
        "Workflow:\n",
        "1.  **Data Preparation**: Processes raw data and engineers features as in v7. It also\n",
        "    identifies and separates \"leaked\" sessions present in both train and test sets.\n",
        "2.  **Teacher Model Training**: A baseline CatBoost model is trained on the clean\n",
        "    (non-leaked) training data.\n",
        "3.  **Pseudo-Labeling HPO**: Optuna is used to find the best hyperparameters for the\n",
        "    student model's training process. This HPO tunes the *weight* of pseudo-labels\n",
        "    and a *filtering threshold*, not the model's architecture.\n",
        "4.  **Final Student Model Training**: Using the best HPO parameters, a final student\n",
        "    model is trained on the combination of clean training data and high-quality\n",
        "    pseudo-labeled test data.\n",
        "5.  **Feature Selection**: Feature importance is calculated from the final student model,\n",
        "    and a new, leaner model is trained using only the most impactful features.\n",
        "6.  **Submission Generation**: The final predictions are made using the feature-selected\n",
        "    student model. The predictions for the \"leaked\" sessions are then overwritten\n",
        "    with their known true values to maximize accuracy.\n",
        "\"\"\"\n",
        "\n",
        "# ==============================================================================\n",
        "# 0. SETUP AND IMPORTS\n",
        "# ==============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import gc\n",
        "import json\n",
        "import optuna\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "print(\"Script V8: Full Workflow with Pseudo-Labeling Initiated.\")\n",
        "\n",
        "# --- Configuration ---\n",
        "# Input Paths\n",
        "IN_TRAIN_PATH = '/content/datathon/train.csv'\n",
        "IN_TEST_PATH = '/content/datathon/test.csv'\n",
        "SAMPLE_SUBMISSION_PATH = '/content/datathon/sample_submission.csv'\n",
        "\n",
        "# Output Directories\n",
        "OUT_DIR = '/content/datathon/processed/'\n",
        "MODEL_DIR = \"/content/models/V8/\"\n",
        "SUBMISSION_DIR = '/content/submissions/'\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "os.makedirs(SUBMISSION_DIR, exist_ok=True)\n",
        "\n",
        "# Processed File Paths\n",
        "OUT_TRAIN_PATH = os.path.join(OUT_DIR, 'train_processed_v8.csv')\n",
        "OUT_TEST_PATH = os.path.join(OUT_DIR, 'test_processed_v8.csv')\n",
        "\n",
        "# Model & Submission Paths\n",
        "TEACHER_MODEL_PATH = os.path.join(MODEL_DIR, \"catboost_model_v8_teacher.cbm\")\n",
        "STUDENT_MODEL_PATH = os.path.join(MODEL_DIR, \"catboost_model_v8_student_selected.cbm\")\n",
        "SUBMISSION_FILE = os.path.join(SUBMISSION_DIR, 'submission_v8.csv')\n",
        "\n",
        "# HPO Database\n",
        "DB_FILENAME = \"optuna_studies_v8.db\"\n",
        "STUDY_NAME = \"catboost_v8_pseudo_labeling\"\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. DATA PREPARATION & LEAK IDENTIFICATION\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Adım 1: Veri Hazırlama ve Sızıntı Tespiti Başladı ---\")\n",
        "\n",
        "# --- Load Raw Data ---\n",
        "try:\n",
        "    df_train_raw = pd.read_csv(IN_TRAIN_PATH, parse_dates=['event_time'])\n",
        "    df_test_raw = pd.read_csv(IN_TEST_PATH, parse_dates=['event_time'])\n",
        "    print(\"Ham veri setleri başarıyla yüklendi.\")\n",
        "except Exception as e:\n",
        "    print(f\"Hata: Ham veri yüklenemedi. {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- Identify Leaked Sessions ---\n",
        "print(\"Ortak (sızıntılı) seanslar tespit ediliyor...\")\n",
        "train_session_users = df_train_raw.groupby('user_session')['user_id'].apply(set)\n",
        "test_session_users = df_test_raw.groupby('user_session')['user_id'].apply(set)\n",
        "common_sessions = set(train_session_users.index).intersection(set(test_session_users.index))\n",
        "verified_leaked_sessions = {sid for sid in common_sessions if train_session_users.get(sid) == test_session_users.get(sid)}\n",
        "\n",
        "# Create a map of leaked session_id -> session_value\n",
        "leak_map = df_train_raw[\n",
        "    df_train_raw['user_session'].isin(verified_leaked_sessions)\n",
        "].groupby('user_session')['session_value'].first().to_dict()\n",
        "print(f\"Tespit edilen DOĞRULANMIŞ sızıntı seans sayısı: {len(verified_leaked_sessions)}\")\n",
        "\n",
        "\n",
        "# --- Feature Engineering (Based on v7) ---\n",
        "def fix_anomalous_sessions(df):\n",
        "    session_user_counts = df.groupby('user_session')['user_id'].nunique()\n",
        "    anomalous_sessions = session_user_counts[session_user_counts > 1].index\n",
        "    if not anomalous_sessions.empty:\n",
        "        anomalous_indices = df['user_session'].isin(anomalous_sessions)\n",
        "        # Use .loc to avoid SettingWithCopyWarning\n",
        "        df_copy = df.copy()\n",
        "        df_copy.loc[anomalous_indices, 'user_session'] = df_copy.loc[anomalous_indices, 'user_session'] + '_' + df_copy.loc[anomalous_indices, 'user_id'].astype(str)\n",
        "        return df_copy\n",
        "    return df\n",
        "\n",
        "df_train_raw_fixed = fix_anomalous_sessions(df_train_raw)\n",
        "df_test_raw_fixed = fix_anomalous_sessions(df_test_raw)\n",
        "\n",
        "df_combined = pd.concat([df_train_raw_fixed.drop('session_value', axis=1), df_test_raw_fixed], ignore_index=True)\n",
        "\n",
        "# User-level features\n",
        "user_features = df_combined.groupby('user_id').agg(\n",
        "    user_total_events=('event_type', 'count'),\n",
        "    user_unique_products_viewed=('product_id', 'nunique'),\n",
        "    user_first_seen=('event_time', 'min'),\n",
        "    user_last_seen=('event_time', 'max')\n",
        ")\n",
        "user_features['user_lifespan_days'] = (user_features['user_last_seen'] - user_features['user_first_seen']).dt.days\n",
        "user_buy_counts = df_combined[df_combined['event_type'] == 'BUY'].groupby('user_id').size()\n",
        "user_features['user_buy_count'] = user_buy_counts\n",
        "user_features['user_buy_count'].fillna(0, inplace=True)\n",
        "user_features['user_purchase_rate'] = user_features['user_buy_count'] / user_features['user_total_events']\n",
        "user_features.drop(['user_first_seen', 'user_last_seen'], axis=1, inplace=True)\n",
        "del df_combined\n",
        "gc.collect()\n",
        "\n",
        "def create_session_features(df, data_type='train'):\n",
        "    print(f\"{data_type} verisi için seans bazlı özellik mühendisliği...\")\n",
        "    df['event_order'] = df.groupby('user_session').cumcount() + 1\n",
        "    session_event_counts = df.groupby('user_session')['event_type'].transform('count')\n",
        "    df['event_order_pct'] = df['event_order'] / session_event_counts\n",
        "\n",
        "    event_type_counts = pd.crosstab(df['user_session'], df['event_type'])\n",
        "    all_event_types = ['VIEW', 'ADD_CART', 'REMOVE_CART', 'BUY']\n",
        "    for event in all_event_types:\n",
        "        if event not in event_type_counts.columns:\n",
        "            event_type_counts[event] = 0\n",
        "    event_type_counts.columns = [f'{col.lower()}_count' for col in event_type_counts.columns]\n",
        "\n",
        "    session_features = df.groupby('user_session').agg(\n",
        "        user_id=('user_id', 'first'),\n",
        "        event_count=('event_type', 'count'),\n",
        "        unique_products=('product_id', 'nunique'),\n",
        "        unique_categories=('category_id', 'nunique'),\n",
        "        session_duration_seconds=('event_time', lambda x: (x.max() - x.min()).total_seconds())\n",
        "    )\n",
        "\n",
        "    df_session = pd.concat([session_features, event_type_counts], axis=1)\n",
        "\n",
        "    epsilon = 1e-6\n",
        "    df_session['view_to_add_cart_rate'] = df_session['add_cart_count'] / (df_session['view_count'] + epsilon)\n",
        "    df_session['add_cart_to_buy_rate'] = df_session['buy_count'] / (df_session['add_cart_count'] + epsilon)\n",
        "    df_session['net_cart_additions'] = df_session['add_cart_count'] - df_session['remove_cart_count']\n",
        "    \n",
        "    # Merge with user features\n",
        "    df_session = df_session.merge(user_features, on='user_id', how='left')\n",
        "    df_session.drop('user_id', axis=1, inplace=True)\n",
        "    \n",
        "    return df_session\n",
        "\n",
        "df_session_train = create_session_features(df_train_raw_fixed, 'train')\n",
        "df_session_test = create_session_features(df_test_raw_fixed, 'test')\n",
        "\n",
        "# Add target variable to train set\n",
        "session_value = df_train_raw_fixed.groupby('user_session')['session_value'].first()\n",
        "df_session_train['session_value'] = session_value\n",
        "\n",
        "# --- Separate Leaked Data from Training Set ---\n",
        "df_session_train_clean = df_session_train[~df_session_train.index.isin(verified_leaked_sessions)]\n",
        "print(f\"Orijinal train seti boyutu: {len(df_session_train)}\")\n",
        "print(f\"Sızıntılı veriler ayıklandıktan sonraki train seti boyutu: {len(df_session_train_clean)}\")\n",
        "\n",
        "# Align columns - crucial for combining later\n",
        "train_cols = df_session_train_clean.drop('session_value', axis=1).columns\n",
        "test_cols = df_session_test.columns\n",
        "common_cols = list(set(train_cols) & set(test_cols))\n",
        "df_session_train_clean = df_session_train_clean[common_cols + ['session_value']]\n",
        "df_session_test = df_session_test[common_cols]\n",
        "\n",
        "print(\"Adım 1 Tamamlandı: Veri işlendi ve sızıntılı seanslar eğitimden ayrıldı.\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. TEACHER MODEL TRAINING\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Adım 2: Teacher Model Eğitimi Başladı ---\")\n",
        "\n",
        "# Prepare data for the teacher model\n",
        "y_teacher = df_session_train_clean['session_value']\n",
        "X_teacher = df_session_train_clean.drop('session_value', axis=1)\n",
        "y_teacher_log = np.log1p(y_teacher)\n",
        "\n",
        "# Using pre-optimized parameters from v7\n",
        "teacher_params = {\n",
        "    'learning_rate': 0.06007502637954865,\n",
        "    'depth': 4,\n",
        "    'l2_leaf_reg': 2.5440079813104126,\n",
        "    'colsample_bylevel': 0.8424763199702767,\n",
        "    'min_child_samples': 29,\n",
        "    'objective': 'RMSE',\n",
        "    'random_seed': 42,\n",
        "    'verbose': 0 # Silent during HPO runs\n",
        "}\n",
        "\n",
        "teacher_model = CatBoostRegressor(\n",
        "    **teacher_params,\n",
        "    iterations=4500 # A sufficiently large number\n",
        ")\n",
        "\n",
        "# Time-based split for early stopping validation\n",
        "X_train_teacher, X_val_teacher, y_train_teacher_log, y_val_teacher_log = train_test_split(\n",
        "    X_teacher, y_teacher_log, test_size=0.2, shuffle=False\n",
        ")\n",
        "\n",
        "print(\"Teacher model eğitiliyor...\")\n",
        "teacher_model.fit(\n",
        "    X_train_teacher, y_train_teacher_log,\n",
        "    eval_set=(X_val_teacher, y_val_teacher_log),\n",
        "    early_stopping_rounds=300,\n",
        "    verbose=1000\n",
        ")\n",
        "\n",
        "teacher_model.save_model(TEACHER_MODEL_PATH)\n",
        "print(f\"Teacher model eğitildi ve '{TEACHER_MODEL_PATH}' olarak kaydedildi.\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. HYPERPARAMETER OPTIMIZATION FOR PSEUDO-LABELING\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Adım 3: Pseudo-Labeling için HPO Başladı ---\")\n",
        "\n",
        "# The data for HPO comes from the clean training set\n",
        "X_hpo = X_teacher\n",
        "y_hpo_log = y_teacher_log\n",
        "X_test_processed = df_session_test\n",
        "\n",
        "# This validation set will be used inside Optuna to get a realistic score\n",
        "X_train_hpo, X_val_hpo, y_train_hpo_log, y_val_hpo_log = train_test_split(\n",
        "    X_hpo, y_hpo_log, test_size=0.2, shuffle=False\n",
        ")\n",
        "\n",
        "def objective(trial):\n",
        "    # --- 1. Define HPO parameters for the pseudo-labeling process ---\n",
        "    pseudo_label_weight = trial.suggest_float('pseudo_label_weight', 0.1, 1.0)\n",
        "    pseudo_label_quantile_filter = trial.suggest_float('pseudo_label_quantile_filter', 0.0, 0.1) # Filter 0% to 20% of data (10% from each tail)\n",
        "\n",
        "    # --- 2. Generate and filter pseudo-labels ---\n",
        "    test_preds_log = teacher_model.predict(X_test_processed)\n",
        "    \n",
        "    df_pseudo = pd.DataFrame({\n",
        "        'pseudo_label_log': test_preds_log\n",
        "    }, index=X_test_processed.index)\n",
        "\n",
        "    # Filtering\n",
        "    if pseudo_label_quantile_filter > 0:\n",
        "        lower_bound = df_pseudo['pseudo_label_log'].quantile(pseudo_label_quantile_filter)\n",
        "        upper_bound = df_pseudo['pseudo_label_log'].quantile(1 - pseudo_label_quantile_filter)\n",
        "        df_pseudo = df_pseudo[(df_pseudo['pseudo_label_log'] >= lower_bound) & (df_pseudo['pseudo_label_log'] <= upper_bound)]\n",
        "\n",
        "    X_test_pseudo = X_test_processed.loc[df_pseudo.index]\n",
        "    y_test_pseudo_log = df_pseudo['pseudo_label_log']\n",
        "\n",
        "    # --- 3. Create the student training set ---\n",
        "    # Combine original training data with pseudo-labeled data\n",
        "    X_student_train = pd.concat([X_train_hpo, X_test_pseudo], axis=0)\n",
        "    y_student_train_log = pd.concat([y_train_hpo_log, y_test_pseudo_log], axis=0)\n",
        "    \n",
        "    # Create sample weights: 1 for real data, tuned weight for pseudo-data\n",
        "    weights = np.concatenate([\n",
        "        np.ones(len(X_train_hpo)),\n",
        "        np.full(len(X_test_pseudo), pseudo_label_weight)\n",
        "    ])\n",
        "\n",
        "    # --- 4. Train and evaluate the student model ---\n",
        "    student_model = CatBoostRegressor(**teacher_params, iterations=3000)\n",
        "    student_model.fit(\n",
        "        X_student_train, y_student_train_log,\n",
        "        sample_weight=weights,\n",
        "        eval_set=(X_val_hpo, y_val_hpo_log), # Evaluate on unseen REAL data\n",
        "        early_stopping_rounds=150,\n",
        "        verbose=0\n",
        "    )\n",
        "    \n",
        "    # --- 5. Calculate final score ---\n",
        "    val_preds_log = student_model.predict(X_val_hpo)\n",
        "    y_val_original = np.expm1(y_val_hpo_log)\n",
        "    val_preds_original = np.expm1(val_preds_log)\n",
        "    val_preds_original[val_preds_original < 0] = 0\n",
        "    \n",
        "    mse = mean_squared_error(y_val_original, val_preds_original)\n",
        "    \n",
        "    gc.collect()\n",
        "    return mse\n",
        "\n",
        "# --- Run the HPO study ---\n",
        "storage_name = f\"sqlite:///{DB_FILENAME}\"\n",
        "study = optuna.create_study(\n",
        "    study_name=STUDY_NAME,\n",
        "    storage=storage_name,\n",
        "    direction='minimize',\n",
        "    load_if_exists=True\n",
        ")\n",
        "\n",
        "print(f\"Optimizasyon başlıyor... Sonuçlar '{DB_FILENAME}' dosyasına kaydedilecek.\")\n",
        "study.optimize(objective, n_trials=30) # Run for 30 trials\n",
        "\n",
        "best_pseudo_params = study.best_params\n",
        "print(\"\\nOptimizasyon Tamamlandı!\")\n",
        "print(f\"En iyi denemenin skoru (MSE): {study.best_value}\")\n",
        "print(\"En iyi pseudo-labeling parametreleri:\")\n",
        "print(best_pseudo_params)\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. FINAL STUDENT MODEL TRAINING & FEATURE SELECTION\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Adım 4: Final Student Model Eğitimi ve Özellik Seçimi Başladı ---\")\n",
        "\n",
        "# --- 4a. Generate final pseudo-labels with best params ---\n",
        "print(\"En iyi parametrelerle final pseudo-label'lar oluşturuluyor...\")\n",
        "final_quantile = best_pseudo_params['pseudo_label_quantile_filter']\n",
        "final_weight = best_pseudo_params['pseudo_label_weight']\n",
        "\n",
        "test_preds_log = teacher_model.predict(X_test_processed)\n",
        "df_pseudo_final = pd.DataFrame({'pseudo_label_log': test_preds_log}, index=X_test_processed.index)\n",
        "\n",
        "if final_quantile > 0:\n",
        "    lower_bound = df_pseudo_final['pseudo_label_log'].quantile(final_quantile)\n",
        "    upper_bound = df_pseudo_final['pseudo_label_log'].quantile(1 - final_quantile)\n",
        "    df_pseudo_final = df_pseudo_final[(df_pseudo_final['pseudo_label_log'] >= lower_bound) & (df_pseudo_final['pseudo_label_log'] <= upper_bound)]\n",
        "\n",
        "X_test_pseudo_final = X_test_processed.loc[df_pseudo_final.index]\n",
        "y_test_pseudo_final_log = df_pseudo_final['pseudo_label_log']\n",
        "\n",
        "# --- 4b. Create final combined training set ---\n",
        "# Use ALL clean training data this time\n",
        "X_student_final = pd.concat([X_teacher, X_test_pseudo_final], axis=0)\n",
        "y_student_final_log = pd.concat([y_teacher_log, y_test_pseudo_final_log], axis=0)\n",
        "final_weights = np.concatenate([\n",
        "    np.ones(len(X_teacher)),\n",
        "    np.full(len(X_test_pseudo_final), final_weight)\n",
        "])\n",
        "\n",
        "# --- 4c. Train the student model on ALL data to get feature importances ---\n",
        "print(\"Özellik önem skorlarını almak için final student model eğitiliyor...\")\n",
        "student_model_for_fi = CatBoostRegressor(\n",
        "    **teacher_params,\n",
        "    iterations=teacher_model.get_best_iteration() # Use teacher's best iteration\n",
        ")\n",
        "student_model_for_fi.fit(\n",
        "    X_student_final, y_student_final_log,\n",
        "    sample_weight=final_weights,\n",
        "    verbose=1000\n",
        ")\n",
        "\n",
        "# --- 4d. Perform feature selection ---\n",
        "fi = student_model_for_fi.get_feature_importance()\n",
        "df_fi = pd.DataFrame({\"feature\": X_student_final.columns, \"importance\": fi})\n",
        "df_fi = df_fi.sort_values(\"importance\", ascending=False)\n",
        "\n",
        "importance_threshold = 0.2\n",
        "selected_features = df_fi[df_fi['importance'] >= importance_threshold]['feature'].tolist()\n",
        "print(f\"Toplam {len(df_fi)} özellikten, önemi >= {importance_threshold} olan {len(selected_features)} adet özellik seçildi.\")\n",
        "\n",
        "# --- 4e. Train final, feature-selected student model ---\n",
        "print(\"Seçilmiş özelliklerle nihai student model eğitiliyor...\")\n",
        "X_student_selected = X_student_final[selected_features]\n",
        "\n",
        "final_student_model = CatBoostRegressor(\n",
        "    **teacher_params,\n",
        "    iterations=teacher_model.get_best_iteration() + 500 # Add a few more iterations\n",
        ")\n",
        "final_student_model.fit(\n",
        "    X_student_selected, y_student_final_log,\n",
        "    sample_weight=final_weights,\n",
        "    verbose=1000\n",
        ")\n",
        "\n",
        "final_student_model.save_model(STUDENT_MODEL_PATH)\n",
        "print(f\"Final Student model eğitildi ve '{STUDENT_MODEL_PATH}' olarak kaydedildi.\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. SUBMISSION GENERATION\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Adım 5: Submission Dosyası Oluşturuluyor ---\")\n",
        "\n",
        "# --- 5a. Load necessary components ---\n",
        "df_submission = pd.read_csv(SAMPLE_SUBMISSION_PATH)\n",
        "model = CatBoostRegressor()\n",
        "model.load_model(STUDENT_MODEL_PATH)\n",
        "\n",
        "# --- 5b. Make predictions with the final model ---\n",
        "print(\"Test seti üzerinde tahminler yapılıyor...\")\n",
        "X_test_selected = df_session_test[selected_features]\n",
        "test_preds_log = model.predict(X_test_selected)\n",
        "final_predictions = np.expm1(test_preds_log)\n",
        "final_predictions[final_predictions < 0] = 0\n",
        "\n",
        "df_predictions = pd.DataFrame({\n",
        "    'user_session': X_test_selected.index,\n",
        "    'predicted_value': final_predictions\n",
        "})\n",
        "\n",
        "# --- 5c. Fill submission with model predictions ---\n",
        "submission_map = dict(zip(df_predictions['user_session'], df_predictions['predicted_value']))\n",
        "df_submission['session_value'] = df_submission['user_session'].map(submission_map)\n",
        "\n",
        "# --- 5d. Handle anomalous sessions (smart filling) ---\n",
        "session_user_counts = df_test_raw.groupby('user_session')['user_id'].nunique()\n",
        "anomalous_sessions_orig = session_user_counts[session_user_counts > 1].index.tolist()\n",
        "if anomalous_sessions_orig:\n",
        "    print(f\"\\n{len(anomalous_sessions_orig)} adet anormal seans için akıllı doldurma yapılıyor...\")\n",
        "    for session_id in anomalous_sessions_orig:\n",
        "        constituent_preds = df_predictions[df_predictions['user_session'].str.startswith(f\"{session_id}_\")]\n",
        "        if not constituent_preds.empty:\n",
        "            total_value = constituent_preds['predicted_value'].sum()\n",
        "            df_submission.loc[df_submission['user_session'] == session_id, 'session_value'] = total_value\n",
        "\n",
        "# --- 5e. Overwrite with leaked data (CRITICAL STEP) ---\n",
        "print(f\"\\n{len(leak_map)} adet seansın değeri, sızıntıdan gelen gerçek değerlerle güncelleniyor...\")\n",
        "leaked_updates = df_submission['user_session'].map(leak_map)\n",
        "df_submission['session_value'] = np.where(leaked_updates.notna(), leaked_updates, df_submission['session_value'])\n",
        "\n",
        "# --- 5f. Final checks and save ---\n",
        "nan_count = df_submission['session_value'].isnull().sum()\n",
        "if nan_count > 0:\n",
        "    print(f\"UYARI: Hala {nan_count} adet null değer var! Bunlar 0 ile dolduruluyor.\")\n",
        "    df_submission['session_value'].fillna(0, inplace=True)\n",
        "\n",
        "df_submission.to_csv(SUBMISSION_FILE, index=False)\n",
        "print(f\"\\n'{SUBMISSION_FILE}' dosyası başarıyla oluşturuldu!\")\n",
        "print(\"Dosyanın ilk 5 satırı:\")\n",
        "print(df_submission.head())\n",
        "\n",
        "print(\"\\n--- V8 SÜRECİ TAMAMLANDI ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJS5eu9MxAXI",
        "outputId": "c849458f-336e-4f9d-a000-91d7a10faca3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/datathon-2025.zip\n",
            "  inflating: sample_submission.csv   \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/datathon-2025.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJxKNwa3AAv_"
      },
      "source": [
        "**Veri Hazırlama**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1W-FyupACZ1",
        "outputId": "18a02e27-b09b-4e8b-8911-c6aaf4ef6bb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adım 1 (v8 - Zero-Shot Kategorizasyon ile): Veri Hazırlama Başladı.\n",
            "Ham veri setleri başarıyla yüklendi.\n",
            "Analizler için ön bilgiler hesaplandı.\n",
            "\n",
            "Train ve test verileri birleştirildi.\n",
            "Kullanıcı bazlı özellikler (user_features) oluşturuluyor...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_369415/860223108.py:72: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  user_features['user_buy_count'].fillna(0, inplace=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   -> Kullanıcı bazlı özellikler tamamlandı.\n",
            "\n",
            "Ağırlıklandırılmış kullanıcı tercihlerine göre Zero-Shot kategorizasyon başlıyor...\n",
            "   -> Olay ağırlıkları tanımlandı: {'VIEW': 0.07, 'ADD_CART': 0.18, 'BUY': 0.75, 'REMOVE_CART': -0.18}\n",
            "   -> Ağırlıklandırılmış Kullanıcı-Ürün matrisi oluşturuluyor...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "import os\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "import os\n",
        "\n",
        "print(\"Adım 1 (v8 - Zero-Shot Kategorizasyon ile): Veri Hazırlama Başladı.\")\n",
        "\n",
        "IN_TRAIN_PATH = './train.csv'\n",
        "IN_TEST_PATH = './test.csv'\n",
        "\n",
        "OUT_DIR = '/processed'\n",
        "\n",
        "OUT_TRAIN_PATH = OUT_DIR + 'train_processed_v7.csv'\n",
        "OUT_TEST_PATH = OUT_DIR + 'test_processed_v7.csv'\n",
        "\n",
        "\n",
        "df_train = pd.read_csv(IN_TRAIN_PATH, parse_dates=['event_time'])\n",
        "df_test = pd.read_csv(IN_TEST_PATH, parse_dates=['event_time'])\n",
        "print(\"Ham veri setleri başarıyla yüklendi.\")\n",
        "\n",
        "# Analiz için ön bilgiler\n",
        "train_users = set(df_train['user_id'])\n",
        "test_users = set(df_test['user_id'])\n",
        "common_users = train_users.intersection(test_users)\n",
        "train_products = set(df_train['product_id'])\n",
        "new_products_in_test = set(df_test['product_id']) - train_products\n",
        "train_session_users = df_train.groupby('user_session')['user_id'].apply(set)\n",
        "test_session_users = df_test.groupby('user_session')['user_id'].apply(set)\n",
        "common_sessions = set(train_session_users.index).intersection(set(test_session_users.index))\n",
        "verified_leaked_sessions = {sid for sid in common_sessions if train_session_users[sid] == test_session_users[sid]}\n",
        "print(\"Analizler için ön bilgiler hesaplandı.\")\n",
        "\n",
        "\n",
        "# --- Anomali Temizleme, Kullanıcı Özellikleri (v6 ile aynı) ---\n",
        "# ... (Bu kısımlar önceki script ile birebir aynı, o yüzden tekrar eklemiyorum)\n",
        "def fix_anomalous_sessions(df, data_type='train'):\n",
        "    session_user_counts = df.groupby('user_session')['user_id'].nunique()\n",
        "    anomalous_sessions = session_user_counts[session_user_counts > 1].index\n",
        "    if len(anomalous_sessions) > 0:\n",
        "        anomalous_indices = df['user_session'].isin(anomalous_sessions)\n",
        "        df['user_session_corrected'] = df['user_session']\n",
        "        df.loc[anomalous_indices, 'user_session_corrected'] = df.loc[anomalous_indices, 'user_session'] + '_' + df.loc[anomalous_indices, 'user_id']\n",
        "        df.drop('user_session', axis=1, inplace=True)\n",
        "        df.rename(columns={'user_session_corrected': 'user_session'}, inplace=True)\n",
        "    return df\n",
        "\n",
        "# Anormal seansları düzelt\n",
        "df_train = fix_anomalous_sessions(df_train, 'train')\n",
        "df_test = fix_anomalous_sessions(df_test, 'test')\n",
        "\n",
        "# Train ve Test verilerini birleştirerek genel özellikler için hazırla\n",
        "df_combined = pd.concat([df_train.drop('session_value', axis=1), df_test], ignore_index=True)\n",
        "print(\"\\nTrain ve test verileri birleştirildi.\")\n",
        "\n",
        "# Kullanıcı bazlı özellikleri tüm veriden hesapla\n",
        "print(\"Kullanıcı bazlı özellikler (user_features) oluşturuluyor...\")\n",
        "user_features = df_combined.groupby('user_id').agg(\n",
        "    user_total_events=('event_type', 'count'),\n",
        "    user_unique_products_viewed=('product_id', 'nunique'),\n",
        "    user_first_seen=('event_time', 'min'),\n",
        "    user_last_seen=('event_time', 'max')\n",
        ")\n",
        "user_features['user_lifespan_days'] = (user_features['user_last_seen'] - user_features['user_first_seen']).dt.days\n",
        "user_buy_counts = df_combined[df_combined['event_type'] == 'BUY'].groupby('user_id').size()\n",
        "user_features['user_buy_count'] = user_buy_counts\n",
        "user_features['user_buy_count'].fillna(0, inplace=True)\n",
        "user_features['user_purchase_rate'] = user_features['user_buy_count'] / user_features['user_total_events']\n",
        "user_features.drop(['user_first_seen', 'user_last_seen'], axis=1, inplace=True)\n",
        "print(\"   -> Kullanıcı bazlı özellikler tamamlandı.\")\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "\n",
        "# ------------------------------------------------------------------------------------\n",
        "# --- YENİ ADIM (v8): AĞIRLIKLANDIRILMIŞ KULLANICI TERCİHLERİNE GÖRE ZERO-SHOT KATEGORİZASYON ---\n",
        "# ------------------------------------------------------------------------------------\n",
        "print(\"\\nAğırlıklandırılmış kullanıcı tercihlerine göre Zero-Shot kategorizasyon başlıyor...\")\n",
        "\n",
        "# 1. Her olay türü için bir ağırlık (önem) tanımla.\n",
        "# REMOVE_CART, negatif bir sinyal olduğu için negatif bir ağırlığa sahip.\n",
        "event_weights = {\n",
        "    'VIEW': 0.07,\n",
        "    'ADD_CART': 0.18,\n",
        "    'BUY': 0.75,\n",
        "    'REMOVE_CART': -0.18\n",
        "}\n",
        "print(f\"   -> Olay ağırlıkları tanımlandı: {event_weights}\")\n",
        "\n",
        "# 2. Ağırlıkları kullanarak her bir etkileşim için bir skor oluştur.\n",
        "df_combined['interaction_score'] = df_combined['event_type'].map(event_weights).fillna(0)\n",
        "\n",
        "# 3. Ağırlıklandırılmış Kullanıcı-Ürün matrisini 'pivot_table' ile oluştur.\n",
        "# Her hücre, bir kullanıcının bir ürünle olan toplam etkileşim skorunu içerir.\n",
        "print(\"   -> Ağırlıklandırılmış Kullanıcı-Ürün matrisi oluşturuluyor...\")\n",
        "user_product_matrix = df_combined.pivot_table(\n",
        "    index='user_id',\n",
        "    columns='product_id',\n",
        "    values='interaction_score',\n",
        "    aggfunc='sum'\n",
        ").fillna(0)\n",
        "print(f\"   -> Kullanıcı-Ürün matrisi oluşturuldu. Boyut: {user_product_matrix.shape}\")\n",
        "\n",
        "# 4. Ürünlerin davranışsal vektörlerini (embeddings) SVD ile oluştur\n",
        "N_COMPONENTS_SVD = 100\n",
        "svd = TruncatedSVD(n_components=N_COMPONENTS_SVD, random_state=42)\n",
        "product_embeddings = svd.fit_transform(user_product_matrix) # Matris zaten (kullanıcı x ürün) formatında, SVD'ye bu şekilde verilir.\n",
        "# SVD ürün bazlı vektörler için matrisin transpozunu bekler, biz de (ürün x kullanıcı) matrisinin transpozunu alıyoruz.\n",
        "product_embeddings_transposed = svd.fit_transform(user_product_matrix.T)\n",
        "print(f\"   -> Ürünler için {N_COMPONENTS_SVD} boyutlu davranışsal vektörler SVD ile oluşturuldu.\")\n",
        "\n",
        "# 5. Ürün vektörlerini K-Means ile kümeleyerek yeni kategoriler oluştur\n",
        "N_CLUSTERS_KMEANS = 75\n",
        "kmeans = MiniBatchKMeans(n_clusters=N_CLUSTERS_KMEANS, random_state=42, batch_size=256, n_init='auto')\n",
        "product_clusters = kmeans.fit_predict(product_embeddings_transposed)\n",
        "print(f\"   -> Davranışsal vektörler {N_CLUSTERS_KMEANS} adet kümeye (yeni kategoriye) ayrıldı.\")\n",
        "\n",
        "# 6. Her ürün için oluşturulan yeni kategori ID'sini bir map'e ata\n",
        "product_to_zeroshot_cat = pd.Series(product_clusters, index=user_product_matrix.columns).to_dict()\n",
        "print(\"   -> Ürün -> Zero-Shot Kategori haritası oluşturuldu.\")\n",
        "\n",
        "# Bellek temizliği\n",
        "del user_product_matrix, product_embeddings, product_embeddings_transposed, product_clusters\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "# --- 4. ADIM: SEANS BAZLI ÖZELLİKLER (v8) ---\n",
        "def create_session_features_v8(df, product_to_zeroshot_map, data_type='train'):\n",
        "    print(f\"\\n{data_type} verisi için seans bazlı özellik mühendisliği (v8) başlıyor...\")\n",
        "\n",
        "    # Zero-shot kategorileri ekle\n",
        "    df['zeroshot_category_id'] = df['product_id'].map(product_to_zeroshot_map).fillna(-1).astype(int)\n",
        "\n",
        "    # Seans içi olay sırasını hesapla\n",
        "    df['event_order'] = df.groupby('user_session').cumcount() + 1\n",
        "    session_event_counts = df['user_session'].map(df['user_session'].value_counts())\n",
        "    df['event_order_pct'] = df['event_order'] / session_event_counts\n",
        "\n",
        "    # Olay tiplerini say\n",
        "    event_type_counts = pd.crosstab(df['user_session'], df['event_type'])\n",
        "    all_event_types = ['VIEW', 'ADD_CART', 'REMOVE_CART', 'BUY']\n",
        "    for event in all_event_types:\n",
        "        if event not in event_type_counts.columns:\n",
        "            event_type_counts[event] = 0\n",
        "    event_type_counts.columns = [f'{col.lower()}_count' for col in event_type_counts.columns]\n",
        "\n",
        "    # Popüler kategorileri belirle (orijinal ve zero-shot)\n",
        "    popular_categories = df['category_id'].value_counts().nlargest(int(df['category_id'].nunique() * 0.1)).index\n",
        "    popular_zeroshot_categories = df['zeroshot_category_id'].value_counts().nlargest(int(df['zeroshot_category_id'].nunique() * 0.1)).index\n",
        "\n",
        "    # Seans bazında ana özellikleri topla\n",
        "    session_features = df.groupby('user_session').agg(\n",
        "        user_id=('user_id', 'first'),\n",
        "        event_count=('event_type', 'count'),\n",
        "        unique_products=('product_id', 'nunique'),\n",
        "        unique_categories=('category_id', 'nunique'),\n",
        "        unique_zeroshot_categories=('zeroshot_category_id', 'nunique'),\n",
        "        session_duration_seconds=('event_time', lambda x: (x.max() - x.min()).total_seconds()),\n",
        "        avg_day_of_week=('event_time', lambda x: x.dt.dayofweek.mean()),\n",
        "        avg_hour=('event_time', lambda x: x.dt.hour.mean()),\n",
        "        avg_event_order=('event_order', 'mean'),\n",
        "        avg_event_order_pct=('event_order_pct', 'mean')\n",
        "    )\n",
        "\n",
        "    # Popüler kategori varlık özelliklerini ekle\n",
        "    session_features['has_popular_category'] = df.groupby('user_session')['category_id'].apply(lambda x: 1 if any(cat in popular_categories for cat in x) else 0)\n",
        "    session_features['has_popular_zeroshot_category'] = df.groupby('user_session')['zeroshot_category_id'].apply(lambda x: 1 if any(cat in popular_zeroshot_categories for cat in x) else 0)\n",
        "\n",
        "    # Olay sayılarını ve diğer özellikleri birleştir\n",
        "    df_session = pd.concat([session_features, event_type_counts], axis=1)\n",
        "\n",
        "    # Oran bazlı özellikleri hesapla\n",
        "    epsilon = 1e-6\n",
        "    df_session['view_to_add_cart_rate'] = df_session['add_cart_count'] / (df_session['view_count'] + epsilon)\n",
        "    df_session['add_cart_to_buy_rate'] = df_session['buy_count'] / (df_session['add_cart_count'] + epsilon)\n",
        "    df_session['view_to_buy_rate'] = df_session['buy_count'] / (df_session['view_count'] + epsilon)\n",
        "    df_session['net_cart_additions'] = df_session['add_cart_count'] - df_session['remove_cart_count']\n",
        "    df_session['did_purchase'] = (df_session['buy_count'] > 0).astype(int)\n",
        "\n",
        "    print(f\"   -> {data_type} için seans bazlı özellikler tamamlandı.\")\n",
        "    return df_session\n",
        "\n",
        "# Fonksiyonu çağırarak train ve test için seans bazlı özellikleri oluştur\n",
        "df_session_train = create_session_features_v8(df_train, product_to_zeroshot_cat, 'train')\n",
        "df_session_test = create_session_features_v8(df_test, product_to_zeroshot_cat, 'test')\n",
        "\n",
        "# --- 5. ADIM ve sonrası (v6 ile aynı) ---\n",
        "print(\"\\nKullanıcı ve seans özellikleri birleştiriliyor...\")\n",
        "df_session_train = df_session_train.merge(user_features, on='user_id', how='left').set_index(df_session_train.index)\n",
        "df_session_test = df_session_test.merge(user_features, on='user_id', how='left').set_index(df_session_test.index)\n",
        "df_session_train.drop('user_id', axis=1, inplace=True)\n",
        "df_session_test.drop('user_id', axis=1, inplace=True)\n",
        "\n",
        "print(\"\\nEn önemli etkileşim özellikleri oluşturuluyor...\")\n",
        "for df in [df_session_train, df_session_test]:\n",
        "    df['buy_x_hour'] = df['buy_count'] * df['avg_hour']\n",
        "    df['buy_x_unique_products'] = df['buy_count'] * df['unique_products']\n",
        "    df['buy_x_user_purchase_rate'] = df['buy_count'] * df['user_purchase_rate']\n",
        "\n",
        "session_value = df_train.groupby('user_session')['session_value'].first()\n",
        "df_session_train['session_value'] = session_value\n",
        "\n",
        "df_session_train.to_csv(OUT_TRAIN_PATH)\n",
        "df_session_test.to_csv(OUT_TEST_PATH)\n",
        "\n",
        "print(\"\\nAdım 1 (v7) Tamamlandı: 'train_processed_v7.csv' ve 'test_processed_v7.csv' dosyaları oluşturuldu.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC8dHKh9C_kP"
      },
      "source": [
        "**Model Eğitme**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YryMpU8IDBfJ",
        "outputId": "b7744dcc-a70c-40b9-962e-edb18867abb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adım 2: Model Eğitimi Başlandı.\n",
            "İşlenmiş train verisi ('train_processed.csv') yüklendi.\n",
            "Train seti boyutu: 56605, Validation seti boyutu: 14152\n",
            "CatBoost Modeli eğitimi başlıyor...\n",
            "0:\tlearn: 0.7429778\ttest: 0.7430018\tbest: 0.7430018 (0)\ttotal: 55.4ms\tremaining: 4m 9s\n",
            "500:\tlearn: 0.4376195\ttest: 0.4357696\tbest: 0.4357696 (500)\ttotal: 4.33s\tremaining: 34.6s\n",
            "1000:\tlearn: 0.4346994\ttest: 0.4350890\tbest: 0.4350616 (962)\ttotal: 7.7s\tremaining: 26.9s\n",
            "1500:\tlearn: 0.4328432\ttest: 0.4350377\tbest: 0.4349916 (1413)\ttotal: 11.1s\tremaining: 22.2s\n",
            "Stopped by overfitting detector  (300 iterations wait)\n",
            "\n",
            "bestTest = 0.4349916238\n",
            "bestIteration = 1413\n",
            "\n",
            "Shrink model to first 1414 iterations.\n",
            "\n",
            "Validation Seti Üzerindeki MSE Skoru (CatBoost): 312.8961\n",
            "Validation Seti Üzerindeki RMSE Skoru (CatBoost): 17.6889\n",
            "\n",
            "Final CatBoost modeli tüm train verisi üzerinde eğitiliyor...\n",
            "0:\tlearn: 0.7429769\ttotal: 12ms\tremaining: 17s\n",
            "500:\tlearn: 0.4370777\ttotal: 5.04s\tremaining: 9.18s\n",
            "1000:\tlearn: 0.4344697\ttotal: 9.21s\tremaining: 3.79s\n",
            "1412:\tlearn: 0.4332047\ttotal: 12.7s\tremaining: 0us\n",
            "\n",
            "Adım 2 Tamamlandı: 'catboost_model_v7.cbm' dosyası olarak model kaydedildi.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "print(\"Adım 2: Model Eğitimi Başlandı.\")\n",
        "\n",
        "MODEL_DIR = \"/content/models/V7\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "IN_TRAIN_PATH = \"/content/datathon/processed/train_processed_v7.csv\"\n",
        "\n",
        "FEATURE_IMPORTANCE_DIR_OUT = MODEL_DIR + \"/feature_importance/IN/\"\n",
        "os.makedirs(FEATURE_IMPORTANCE_DIR_OUT, exist_ok=True)\n",
        "\n",
        "OUT_FEATURES_PATH = FEATURE_IMPORTANCE_DIR_OUT + \"importance.json\"\n",
        "\n",
        "# --- İşlenmiş Veriyi Yükleme ---\n",
        "try:\n",
        "    df_train = pd.read_csv(IN_TRAIN_PATH, index_col='user_session')\n",
        "    print(\"İşlenmiş train verisi ('train_processed.csv') yüklendi.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Hata: 'train_processed.csv' bulunamadı. Lütfen önce '1_data_preparation_v7.py' scriptini çalıştırın.\")\n",
        "    exit()\n",
        "\n",
        "# --- Modelleme için Veriyi Hazırlama ---\n",
        "y = df_train['session_value']\n",
        "X = df_train.drop(['session_value'], axis=1)\n",
        "\n",
        "\n",
        "# Hedef değişkene log dönüşümü\n",
        "y_log = np.log1p(y)\n",
        "\n",
        "# --- Zaman Bazlı Doğrulama (Time-Based Validation) ---\n",
        "X_train, X_val, y_train_log, y_val_log = train_test_split(\n",
        "    X, y_log, test_size=0.2, shuffle=False # shuffle=False zaman serisi doğrulama için kritik!\n",
        ")\n",
        "print(f\"Train seti boyutu: {X_train.shape[0]}, Validation seti boyutu: {X_val.shape[0]}\")\n",
        "\n",
        "# --- CatBoost Modelini Eğitme ve Değerlendirme ---\n",
        "print(\"CatBoost Modeli eğitimi başlıyor...\")\n",
        "\n",
        "best_params = {\n",
        "    'learning_rate': 0.06007502637954865,\n",
        "    'depth': 4,\n",
        "    'l2_leaf_reg': 2.5440079813104126,\n",
        "    'colsample_bylevel': 0.8424763199702767,\n",
        "    'min_child_samples': 29,\n",
        "    'objective': 'RMSE',\n",
        "    'random_seed': 42,\n",
        "    'verbose': 500\n",
        "}\n",
        "\n",
        "cat_model = CatBoostRegressor(\n",
        "    **best_params,\n",
        "    iterations=4500,\n",
        "    eval_metric='RMSE',\n",
        "    early_stopping_rounds=300\n",
        ")\n",
        "\n",
        "\n",
        "cat_model.fit(\n",
        "    X_train, y_train_log,\n",
        "    eval_set=(X_val, y_val_log)\n",
        ")\n",
        "\n",
        "# --- Performans Değerlendirme ---\n",
        "val_preds_log = cat_model.predict(X_val)\n",
        "val_preds = np.expm1(val_preds_log)\n",
        "val_preds[val_preds < 0] = 0\n",
        "y_val = np.expm1(y_val_log)\n",
        "\n",
        "validation_mse = mean_squared_error(y_val, val_preds)\n",
        "print(f\"\\nValidation Seti Üzerindeki MSE Skoru (CatBoost): {validation_mse:.4f}\")\n",
        "print(f\"Validation Seti Üzerindeki RMSE Skoru (CatBoost): {np.sqrt(validation_mse):.4f}\")\n",
        "\n",
        "# --- Final Modelini Eğitme ve Kaydetme ---\n",
        "print(\"\\nFinal CatBoost modeli tüm train verisi üzerinde eğitiliyor...\")\n",
        "final_model = CatBoostRegressor(\n",
        "    **best_params,\n",
        "    iterations=cat_model.get_best_iteration()\n",
        ")\n",
        "final_model.fit(X, y_log)\n",
        "\n",
        "features = X.columns.tolist()\n",
        "\n",
        "# Feature importance as a DataFrame and save in a format that works\n",
        "fi = final_model.get_feature_importance(prettified=False)\n",
        "df_fi = pd.DataFrame({\"feature\": features, \"importance\": fi})\n",
        "df_fi = df_fi.sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Prefer parquet, fallback to CSV if parquet not available, always also save JSON\n",
        "\n",
        "df_fi.to_json(OUT_FEATURES_PATH, orient=\"records\")\n",
        "\n",
        "\n",
        "\n",
        "out_dir = MODEL_DIR + \"/\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "out_path = os.path.join(out_dir, \"catboost_model_v7.cbm\")\n",
        "\n",
        "# Modeli kaydetme\n",
        "final_model.save_model(out_path)\n",
        "\n",
        "print(\"\\nAdım 2 Tamamlandı: 'catboost_model_v7.cbm' dosyası olarak model kaydedildi.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZssZFZPoDdvN"
      },
      "source": [
        "**Model Eğitimi - Özellik Seçilimli**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2bSW8QVDhjV",
        "outputId": "87cf1994-0117-46db-c471-12e33ee00466"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adım 2: Model Eğitimi Başlandı.\n",
            "İşlenmiş train verisi ('train_processed.csv') yüklendi.\n",
            "Özellikler yüklendi.\n",
            "Toplam özellik sayısı: 28\n",
            "Seçilen özellik sayısı: 25\n",
            "Train seti boyutu: 56605, Validation seti boyutu: 14152\n",
            "CatBoost Modeli eğitimi başlıyor...\n",
            "0:\tlearn: 0.7428714\ttest: 0.7429129\tbest: 0.7429129 (0)\ttotal: 20.4ms\tremaining: 1m 31s\n",
            "500:\tlearn: 0.4375224\ttest: 0.4355779\tbest: 0.4355758 (496)\ttotal: 4.23s\tremaining: 33.8s\n",
            "1000:\tlearn: 0.4346846\ttest: 0.4349265\tbest: 0.4349229 (998)\ttotal: 7.91s\tremaining: 27.6s\n",
            "1500:\tlearn: 0.4329725\ttest: 0.4348845\tbest: 0.4348817 (1493)\ttotal: 12.3s\tremaining: 24.6s\n",
            "2000:\tlearn: 0.4317747\ttest: 0.4349137\tbest: 0.4348663 (1813)\ttotal: 16s\tremaining: 20s\n",
            "Stopped by overfitting detector  (300 iterations wait)\n",
            "\n",
            "bestTest = 0.4348663493\n",
            "bestIteration = 1813\n",
            "\n",
            "Shrink model to first 1814 iterations.\n",
            "\n",
            "Validation Seti Üzerindeki MSE Skoru (CatBoost): 298.1948\n",
            "Validation Seti Üzerindeki RMSE Skoru (CatBoost): 17.2683\n",
            "\n",
            "Final CatBoost modeli tüm train verisi üzerinde eğitiliyor...\n",
            "0:\tlearn: 0.7428587\ttotal: 10.8ms\tremaining: 19.6s\n",
            "500:\tlearn: 0.4370247\ttotal: 4.44s\tremaining: 11.6s\n",
            "1000:\tlearn: 0.4345215\ttotal: 9.58s\tremaining: 7.77s\n",
            "1500:\tlearn: 0.4330682\ttotal: 13.8s\tremaining: 2.87s\n",
            "1812:\tlearn: 0.4323872\ttotal: 16.6s\tremaining: 0us\n",
            "\n",
            "Adım 2 Tamamlandı: 'catboost_model_v7_selected.cbm' dosyası olarak model kaydedildi.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "print(\"Adım 2: Model Eğitimi Başlandı.\")\n",
        "\n",
        "MODEL_DIR = \"/content/models/V7\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "IN_TRAIN_PATH = \"/content/datathon/processed/train_processed_v7.csv\"\n",
        "\n",
        "FEATURE_IMPORTANCE_DIR_IN = MODEL_DIR + \"/feature_importance/IN/\"\n",
        "os.makedirs(FEATURE_IMPORTANCE_DIR_IN, exist_ok=True)\n",
        "\n",
        "IN_FEATURES_PATH = FEATURE_IMPORTANCE_DIR_IN + \"importance.json\"\n",
        "\n",
        "\n",
        "FEATURE_IMPORTANCE_DIR_OUT = MODEL_DIR + \"/feature_importance/OUT/\"\n",
        "os.makedirs(FEATURE_IMPORTANCE_DIR_OUT, exist_ok=True)\n",
        "\n",
        "OUT_FEATURES_PATH = FEATURE_IMPORTANCE_DIR_OUT + \"importance.json\"\n",
        "# --- İşlenmiş Veriyi Yükleme ---\n",
        "try:\n",
        "    df_train = pd.read_csv(IN_TRAIN_PATH, index_col='user_session')\n",
        "    print(\"İşlenmiş train verisi ('train_processed.csv') yüklendi.\")\n",
        "\n",
        "    with open(IN_FEATURES_PATH, 'r') as f:\n",
        "      features_in = json.load(f)\n",
        "\n",
        "    print(\"Özellikler yüklendi.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Hata: 'train_processed.csv' bulunamadı. Lütfen önce '1_data_preparation.py' scriptini çalıştırın.\")\n",
        "    exit()\n",
        "\n",
        "# --- Modelleme için Veriyi Hazırlama ---\n",
        "y = df_train['session_value']\n",
        "X = df_train.drop(['session_value'], axis=1)\n",
        "\n",
        "\n",
        "# Hedef değişkene log dönüşümü\n",
        "y_log = np.log1p(y)\n",
        "\n",
        "# feature seçilimi\n",
        "importance_threshold = 0.2\n",
        "print(f\"Toplam özellik sayısı: {len(features_in)}\")\n",
        "features_in = [item['feature'] for item in features_in if item['importance'] >= importance_threshold]\n",
        "print(f\"Seçilen özellik sayısı: {len(features_in)}\")\n",
        "X = X[features_in]\n",
        "\n",
        "# --- Zaman Bazlı Doğrulama (Time-Based Validation) ---\n",
        "X_train, X_val, y_train_log, y_val_log = train_test_split(\n",
        "    X, y_log, test_size=0.2, shuffle=False # shuffle=False zaman serisi doğrulama için kritik!\n",
        ")\n",
        "print(f\"Train seti boyutu: {X_train.shape[0]}, Validation seti boyutu: {X_val.shape[0]}\")\n",
        "\n",
        "# --- CatBoost Modelini Eğitme ve Değerlendirme ---\n",
        "print(\"CatBoost Modeli eğitimi başlıyor...\")\n",
        "\n",
        "best_params = {\n",
        "    'learning_rate': 0.06007502637954865,\n",
        "    'depth': 4,\n",
        "    'l2_leaf_reg': 2.5440079813104126,\n",
        "    'colsample_bylevel': 0.8424763199702767,\n",
        "    'min_child_samples': 29,\n",
        "    'objective': 'RMSE',\n",
        "    'random_seed': 42,\n",
        "    'verbose': 500\n",
        "}\n",
        "\n",
        "cat_model = CatBoostRegressor(\n",
        "    **best_params,\n",
        "    iterations=4500,\n",
        "    eval_metric='RMSE',\n",
        "    early_stopping_rounds=300\n",
        ")\n",
        "\n",
        "\n",
        "cat_model.fit(\n",
        "    X_train, y_train_log,\n",
        "    eval_set=(X_val, y_val_log)\n",
        ")\n",
        "\n",
        "# --- Performans Değerlendirme ---\n",
        "val_preds_log = cat_model.predict(X_val)\n",
        "val_preds = np.expm1(val_preds_log)\n",
        "val_preds[val_preds < 0] = 0\n",
        "y_val = np.expm1(y_val_log)\n",
        "\n",
        "validation_mse = mean_squared_error(y_val, val_preds)\n",
        "print(f\"\\nValidation Seti Üzerindeki MSE Skoru (CatBoost): {validation_mse:.4f}\")\n",
        "print(f\"Validation Seti Üzerindeki RMSE Skoru (CatBoost): {np.sqrt(validation_mse):.4f}\")\n",
        "\n",
        "# --- Final Modelini Eğitme ve Kaydetme ---\n",
        "print(\"\\nFinal CatBoost modeli tüm train verisi üzerinde eğitiliyor...\")\n",
        "final_model = CatBoostRegressor(\n",
        "    **best_params,\n",
        "    iterations=cat_model.get_best_iteration()\n",
        ")\n",
        "final_model.fit(X, y_log)\n",
        "\n",
        "features_out = X.columns.tolist()\n",
        "\n",
        "# Feature importance as a DataFrame and save in a format that works\n",
        "fi = final_model.get_feature_importance(prettified=False)\n",
        "df_fi = pd.DataFrame({\"feature\": features_out, \"importance\": fi})\n",
        "df_fi = df_fi.sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Prefer parquet, fallback to CSV if parquet not available, always also save JSON\n",
        "\n",
        "df_fi.to_json(OUT_FEATURES_PATH, orient=\"records\")\n",
        "\n",
        "\n",
        "\n",
        "out_dir = \"/content/models/V7/\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "out_path = os.path.join(out_dir, \"catboost_model_v7_selected_1.cbm\")\n",
        "\n",
        "# Modeli kaydetme\n",
        "final_model.save_model(out_path)\n",
        "\n",
        "print(\"\\nAdım 2 Tamamlandı: 'catboost_model_v7_selected.cbm' dosyası olarak model kaydedildi.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u84hJvAdEdgm"
      },
      "source": [
        "**Submission Oluşturma**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RX-XL-DZW2H5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "print(\"Adım 3 (Akıllı Doldurma ile): Tahmin ve Gönderim Başladı.\")\n",
        "\n",
        "# --- Ayarlar ---\n",
        "PROCESSED_TEST_FILE = '/content/datathon/processed/test_processed_v7.csv'\n",
        "MODEL_FILE = '/content/models/V7/catboost_model_v7_selected_1.cbm' # v3 için optimize edilmiş modeli kullandığımızdan emin olalım\n",
        "SELECTED_FEATURES_PATH = '/content/models/V7/feature_importance/OUT/importance.json'\n",
        "TRAIN_RAW_PATH = '/content/datathon/train.csv' # Sızıntı tespiti için gerekli\n",
        "TEST_RAW_PATH = '/content/datathon/test.csv'   # Sızıntı ve anomali tespiti için gerekli\n",
        "SUBMISSION_FILE = '/content/submissions/submission_v7.csv'\n",
        "\n",
        "\n",
        "# --- Gerekli Dosyaları Yükleme ---\n",
        "try:\n",
        "    df_test_processed = pd.read_csv(PROCESSED_TEST_FILE) # index_col olmadan okuyoruz\n",
        "    df_submission = pd.read_csv('/content/datathon/sample_submission.csv')\n",
        "    df_train_raw = pd.read_csv(TRAIN_RAW_PATH)\n",
        "    df_test_raw = pd.read_csv(TEST_RAW_PATH)\n",
        "\n",
        "    with open(SELECTED_FEATURES_PATH, 'r') as f:\n",
        "      selected_features_dict = json.load(f)\n",
        "\n",
        "    model = CatBoostRegressor()\n",
        "    model.load_model(MODEL_FILE)\n",
        "\n",
        "    print(f\"Gerekli dosyalar ve {len(selected_features_dict)} adet seçilmiş özellik başarıyla yüklendi.\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Hata: {e}.\")\n",
        "    exit()\n",
        "\n",
        "# --- 1. ADIM: DOĞRULANMIŞ VERİ SIZINTISINI TESPİT ET ---\n",
        "print(\"\\nDoğrulanmış veri sızıntısı tespit ediliyor...\")\n",
        "train_session_users = df_train_raw.groupby('user_session')['user_id'].apply(set)\n",
        "test_session_users = df_test_raw.groupby('user_session')['user_id'].apply(set)\n",
        "common_sessions = set(train_session_users.index).intersection(set(test_session_users.index))\n",
        "verified_leaked_sessions = {sid for sid in common_sessions if train_session_users[sid] == test_session_users[sid]}\n",
        "verified_leak_map = df_train_raw[\n",
        "    df_train_raw['user_session'].isin(verified_leaked_sessions)\n",
        "].groupby('user_session')['session_value'].first().to_dict()\n",
        "print(f\"Tespit edilen DOĞRULANMIŞ sızıntı seans sayısı: {len(verified_leaked_sessions)}\")\n",
        "\n",
        "\n",
        "# --- 2. ADIM: MODEL TAHMİNLERİNİ YAP --\n",
        "selected_features = [item['feature'] for item in selected_features_dict]\n",
        "\n",
        "# Olası eksik sütun hatalarını önlemek için kontrol\n",
        "missing_cols = set(selected_features) - set(df_test_processed.columns)\n",
        "if missing_cols:\n",
        "    print(f\"HATA: Test verisinde şu sütunlar eksik: {missing_cols}\")\n",
        "    exit()\n",
        "\n",
        "# Sütun sırasının modelin beklediğiyle aynı olmasını garantile\n",
        "print(\"Selected Feature Sayısı : \", len(selected_features))\n",
        "# Test verisini filtrele\n",
        "df_test_processed.set_index('user_session', inplace=True)\n",
        "df_test_selected = df_test_processed[selected_features]\n",
        "\n",
        "# Tahmin yap\n",
        "print(\"Test seti üzerinde model tahminleri yapılıyor...\")\n",
        "test_preds_log = model.predict(df_test_selected)\n",
        "final_predictions = np.expm1(test_preds_log)\n",
        "final_predictions[final_predictions < 0] = 0\n",
        "df_predictions = pd.DataFrame({\n",
        "    'user_session': df_test_selected.index,\n",
        "    'predicted_value': final_predictions\n",
        "})\n",
        "\n",
        "# --- 3. ADIM: SUBMISSION DOSYASINI OLUŞTURMA ---\n",
        "# Önce normal model tahminlerini map et\n",
        "submission_map = dict(zip(df_predictions['user_session'], df_predictions['predicted_value']))\n",
        "df_submission['session_value'] = df_submission['user_session'].map(submission_map)\n",
        "\n",
        "# Sonra \"Akıllı Doldurma\" ile NaN user anormalliklerini doldur\n",
        "session_user_counts = df_test_raw.groupby('user_session')['user_id'].nunique()\n",
        "anomalous_sessions_orig = session_user_counts[session_user_counts > 1].index.tolist()\n",
        "print(f\"\\nAkıllı doldurma yapılacak anormal seanslar: {anomalous_sessions_orig}\")\n",
        "for session_id in anomalous_sessions_orig:\n",
        "    constituent_preds = df_predictions[df_predictions['user_session'].str.startswith(f\"{session_id}_\")]\n",
        "    total_value = constituent_preds['predicted_value'].sum()\n",
        "    df_submission.loc[df_submission['user_session'] == session_id, 'session_value'] = total_value\n",
        "    print(f\"'{session_id}' için {len(constituent_preds)} parçanın tahmini toplandı: {total_value:.4f}\")\n",
        "\n",
        "# SON VE EN ÖNEMLİ ADIM: Doğrulanmış sızıntıdan gelen gerçek değerlerle tüm tahminleri EZ\n",
        "print(f\"\\n{len(verified_leaked_sessions)} adet seansın değeri, sızıntıdan gelen gerçek değerlerle güncelleniyor...\")\n",
        "leaked_updates = df_submission['user_session'].map(verified_leak_map)\n",
        "df_submission['session_value'] = np.where(leaked_updates.notna(), leaked_updates, df_submission['session_value'])\n",
        "\n",
        "# --- SON KONTROLLER ---\n",
        "nan_count = df_submission['session_value'].isnull().sum()\n",
        "if nan_count > 0:\n",
        "    print(f\"⚠️ UYARI: Hala {nan_count} adet null değer var! Bunlar 0 ile dolduruluyor.\")\n",
        "    df_submission['session_value'].fillna(0, inplace=True)\n",
        "\n",
        "excepted_row = 30789\n",
        "if excepted_row != len(df_submission):\n",
        "    print(f\"Satır sayıları eşleşmiyor. Beklenen : {excepted_row}. Bulunan : {len(df_submission)}\")\n",
        "    exit()\n",
        "\n",
        "# --- Dosyayı Kaydetme ---\n",
        "df_submission.to_csv(SUBMISSION_FILE, index=False)\n",
        "print(f\"\\n'{SUBMISSION_FILE}' dosyası başarıyla oluşturuldu! Bu son gönderim için bol şans!\")\n",
        "print(\"Dosyanın ilk 5 satırı:\")\n",
        "print(df_submission.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGrpUnEVlvfH"
      },
      "source": [
        "**hpo kodu v7**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBH5ZwKYl3xZ",
        "outputId": "3e103c29-41e0-4c1d-cef1-88fa9ef33f44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.43)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.14.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.4 colorlog-6.9.0 optuna-4.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D5sxmWZsE1wy",
        "outputId": "a0f18482-5d6c-49ac-f6c8-b09c408c2c3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adım 2b: Hiperparametre Optimizasyonu (Optuna ile) Başladı.\n",
            "İşlenmiş train verisi ('train_processed_v7.csv') yüklendi.\n",
            "Özelikler Başarıyla yüklendi... 25\n",
            "\n",
            "Özelikler Seçiliyor...\n",
            "Önemi >= 0.2 olan 24 adet özellik seçildi.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-27 13:47:14,838] A new study created in RDB with name: catboost_v7_features\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizasyon başlıyor... Sonuçlar 'optuna_studies.db' dosyasına kaydedilecek.\n",
            "Mevcut deneme sayısı: 0. Toplamda 100 denemeye ulaşılacak.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-27 13:47:34,069] Trial 0 finished with value: 424.1950566097895 and parameters: {'learning_rate': 0.0690116372249165, 'depth': 10, 'l2_leaf_reg': 1.223810248282764, 'colsample_bylevel': 0.5570085575711088, 'min_child_samples': 91}. Best is trial 0 with value: 424.1950566097895.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Trial 0 bitti | MSE: 424.1951 | Learn RMSE: 0.4165 | Val_0 RMSE: 0.4165 | Val_1 RMSE : 0.4363 | İterasyon: 190\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-27 13:47:51,998] Trial 1 finished with value: 294.5126161013067 and parameters: {'learning_rate': 0.04762256092725526, 'depth': 5, 'l2_leaf_reg': 6.006712204320837, 'colsample_bylevel': 0.5572376924145312, 'min_child_samples': 10}. Best is trial 1 with value: 294.5126161013067.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Trial 1 bitti | MSE: 294.5126 | Learn RMSE: 0.4309 | Val_0 RMSE: 0.4309 | Val_1 RMSE : 0.4349 | İterasyon: 1617\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-27 13:48:38,435] Trial 2 finished with value: 387.26699670277344 and parameters: {'learning_rate': 0.011254878453132895, 'depth': 9, 'l2_leaf_reg': 2.315755618424441, 'colsample_bylevel': 0.621938250000565, 'min_child_samples': 44}. Best is trial 1 with value: 294.5126161013067.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Trial 2 bitti | MSE: 387.2670 | Learn RMSE: 0.4270 | Val_0 RMSE: 0.4270 | Val_1 RMSE : 0.4356 | İterasyon: 1866\n",
            "✅ Trial 3 bitti | MSE: 335.9856 | Learn RMSE: 0.4303 | Val_0 RMSE: 0.4303 | Val_1 RMSE : 0.4352 | İterasyon: 554\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-27 13:48:49,097] Trial 3 finished with value: 335.9855506599756 and parameters: {'learning_rate': 0.05254001772004395, 'depth': 7, 'l2_leaf_reg': 7.126594182314087, 'colsample_bylevel': 0.8437375689162578, 'min_child_samples': 25}. Best is trial 1 with value: 294.5126161013067.\n",
            "[I 2025-08-27 13:49:07,575] Trial 4 finished with value: 386.5751591073186 and parameters: {'learning_rate': 0.030356687488645024, 'depth': 9, 'l2_leaf_reg': 1.4697821018404569, 'colsample_bylevel': 0.872290938514591, 'min_child_samples': 97}. Best is trial 1 with value: 294.5126161013067.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Trial 4 bitti | MSE: 386.5752 | Learn RMSE: 0.4255 | Val_0 RMSE: 0.4255 | Val_1 RMSE : 0.4358 | İterasyon: 544\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-27 13:49:21,235] Trial 5 finished with value: 366.2567109940408 and parameters: {'learning_rate': 0.04455503261784065, 'depth': 8, 'l2_leaf_reg': 3.796390195906418, 'colsample_bylevel': 0.7737328798025489, 'min_child_samples': 99}. Best is trial 1 with value: 294.5126161013067.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Trial 5 bitti | MSE: 366.2567 | Learn RMSE: 0.4276 | Val_0 RMSE: 0.4276 | Val_1 RMSE : 0.4352 | İterasyon: 538\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-27 13:49:47,086] Trial 6 finished with value: 334.024477273911 and parameters: {'learning_rate': 0.016929659254169283, 'depth': 7, 'l2_leaf_reg': 1.2974376178553737, 'colsample_bylevel': 0.8478831903864567, 'min_child_samples': 94}. Best is trial 1 with value: 294.5126161013067.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Trial 6 bitti | MSE: 334.0245 | Learn RMSE: 0.4295 | Val_0 RMSE: 0.4295 | Val_1 RMSE : 0.4351 | İterasyon: 1784\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-27 13:50:09,282] Trial 7 finished with value: 326.2092655378106 and parameters: {'learning_rate': 0.026809139094762388, 'depth': 5, 'l2_leaf_reg': 1.5266608041944536, 'colsample_bylevel': 0.6642769931602601, 'min_child_samples': 29}. Best is trial 1 with value: 294.5126161013067.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Trial 7 bitti | MSE: 326.2093 | Learn RMSE: 0.4315 | Val_0 RMSE: 0.4315 | Val_1 RMSE : 0.4348 | İterasyon: 2193\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-27 13:50:46,561] Trial 8 finished with value: 326.1219444083908 and parameters: {'learning_rate': 0.010346913961916863, 'depth': 6, 'l2_leaf_reg': 1.4987365092626603, 'colsample_bylevel': 0.5207430298903617, 'min_child_samples': 39}. Best is trial 1 with value: 294.5126161013067.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Trial 8 bitti | MSE: 326.1219 | Learn RMSE: 0.4319 | Val_0 RMSE: 0.4319 | Val_1 RMSE : 0.4350 | İterasyon: 3493\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-27 13:51:01,738] Trial 9 finished with value: 383.2198065567333 and parameters: {'learning_rate': 0.04803154803748883, 'depth': 9, 'l2_leaf_reg': 2.0142473774806797, 'colsample_bylevel': 0.7293857083599191, 'min_child_samples': 41}. Best is trial 1 with value: 294.5126161013067.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Trial 9 bitti | MSE: 383.2198 | Learn RMSE: 0.4226 | Val_0 RMSE: 0.4226 | Val_1 RMSE : 0.4358 | İterasyon: 411\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-27 13:51:14,238] Trial 10 finished with value: 313.6754275995975 and parameters: {'learning_rate': 0.09987091512997025, 'depth': 4, 'l2_leaf_reg': 8.301458108375321, 'colsample_bylevel': 0.6226250000013146, 'min_child_samples': 10}. Best is trial 1 with value: 294.5126161013067.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Trial 10 bitti | MSE: 313.6754 | Learn RMSE: 0.4324 | Val_0 RMSE: 0.4324 | Val_1 RMSE : 0.4350 | İterasyon: 1068\n",
            "✅ Trial 11 bitti | MSE: 296.6241 | Learn RMSE: 0.4332 | Val_0 RMSE: 0.4332 | Val_1 RMSE : 0.4347 | İterasyon: 834\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-27 13:51:23,420] Trial 11 finished with value: 296.6240646657087 and parameters: {'learning_rate': 0.09403830358762762, 'depth': 4, 'l2_leaf_reg': 9.803417526175123, 'colsample_bylevel': 0.9965496943006595, 'min_child_samples': 8}. Best is trial 1 with value: 294.5126161013067.\n",
            "[I 2025-08-27 13:51:31,945] Trial 12 finished with value: 298.2963269053093 and parameters: {'learning_rate': 0.09792927575823643, 'depth': 4, 'l2_leaf_reg': 5.372253462885506, 'colsample_bylevel': 0.9868311635554933, 'min_child_samples': 7}. Best is trial 1 with value: 294.5126161013067.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Trial 12 bitti | MSE: 298.2963 | Learn RMSE: 0.4332 | Val_0 RMSE: 0.4332 | Val_1 RMSE : 0.4350 | İterasyon: 754\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-27 13:51:43,996] Trial 13 finished with value: 289.44557895989374 and parameters: {'learning_rate': 0.06795080918579646, 'depth': 5, 'l2_leaf_reg': 9.127221293269104, 'colsample_bylevel': 0.9983716266050546, 'min_child_samples': 71}. Best is trial 13 with value: 289.44557895989374.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Trial 13 bitti | MSE: 289.4456 | Learn RMSE: 0.4309 | Val_0 RMSE: 0.4309 | Val_1 RMSE : 0.4350 | İterasyon: 1091\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-27 13:51:53,691] Trial 14 finished with value: 337.864524345192 and parameters: {'learning_rate': 0.06432548673714628, 'depth': 6, 'l2_leaf_reg': 5.455690076321882, 'colsample_bylevel': 0.9169656731744283, 'min_child_samples': 70}. Best is trial 13 with value: 289.44557895989374.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Trial 14 bitti | MSE: 337.8645 | Learn RMSE: 0.4306 | Val_0 RMSE: 0.4306 | Val_1 RMSE : 0.4352 | İterasyon: 622\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[W 2025-08-27 13:51:58,519] Trial 15 failed with parameters: {'learning_rate': 0.040136280194092384, 'depth': 5, 'l2_leaf_reg': 4.06408180607236, 'colsample_bylevel': 0.7415245081318025, 'min_child_samples': 67} because of the following error: KeyboardInterrupt('').\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-4071707534.py\", line 65, in objective\n",
            "    model.fit(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/catboost/core.py\", line 5873, in fit\n",
            "    return self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/catboost/core.py\", line 2410, in _fit\n",
            "    self._train(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/catboost/core.py\", line 1790, in _train\n",
            "    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n",
            "  File \"_catboost.pyx\", line 5023, in _catboost._CatBoost._train\n",
            "  File \"_catboost.pyx\", line 5072, in _catboost._CatBoost._train\n",
            "KeyboardInterrupt\n",
            "[W 2025-08-27 13:51:58,523] Trial 15 failed with value None.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4071707534.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Optimizasyon başlıyor... Sonuçlar '{DB_FILENAME}' dosyasına kaydedilecek.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Mevcut deneme sayısı: {len(study.trials)}. Toplamda {n_trials} denemeye ulaşılacak.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;31m# --- Sonuçları Yazdırma ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    488\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \"\"\"\n\u001b[0;32m--> 490\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    491\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     ):\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4071707534.py\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCatBoostRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# Modeli eğitirken hem train (X_train) hem de val (X_val) setini izlemesini sağlıyoruz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     model.fit(\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_log\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_log\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_log\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# İki seti de ekledik\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, graph, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   5871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'loss_function'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5872\u001b[0m             \u001b[0mCatBoostRegressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_is_compatible_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss_function'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5873\u001b[0;31m         return self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline,\n\u001b[0m\u001b[1;32m   5874\u001b[0m                          \u001b[0muse_best_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5875\u001b[0m                          \u001b[0mverbose_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   2408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2409\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mplot_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Training plots'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_get_train_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2410\u001b[0;31m                 self._train(\n\u001b[0m\u001b[1;32m   2411\u001b[0m                     \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2412\u001b[0m                     \u001b[0mtrain_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eval_sets\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[1;32m   1788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1790\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minit_model\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1791\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_trained_model_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import optuna\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from catboost import CatBoostRegressor\n",
        "import gc\n",
        "\n",
        "print(\"Adım 2b: Hiperparametre Optimizasyonu (Optuna ile) Başladı.\")\n",
        "\n",
        "# --- Veritabanı ve Çalışma Ayarları ---\n",
        "DB_FILENAME = \"optuna_studies.db\"\n",
        "STUDY_NAME = \"catboost_v7_features\" # Her yeni özellik seti için bu ismi değiştirebilirsin\n",
        "\n",
        "train_path = \"/content/datathon/processed/train_processed_v7.csv\"\n",
        "\n",
        "feature_path = \"/content/models/V7/feature_importance/OUT/importance.json\"\n",
        "\n",
        "# --- Modelleme için Veriyi Hazırlama ---\n",
        "# --- İşlenmiş Veriyi Yükleme ---\n",
        "try:\n",
        "    df_train = pd.read_csv(train_path, index_col='user_session')\n",
        "    print(\"İşlenmiş train verisi ('train_processed_v7.csv') yüklendi.\")\n",
        "\n",
        "    with open(feature_path, 'r') as f:\n",
        "      features_importance = json.load(f)\n",
        "    print(f\"Özelikler Başarıyla yüklendi... {len(features_importance)}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Hata: 'train_processed_v7.csv' bulunamadı. Lütfen önce '1_data_preparation_v7.py' scriptini çalıştırın.\")\n",
        "    exit()\n",
        "\n",
        "# --- 1. Adım: Özellik Seçilimi ---\n",
        "print(\"\\nÖzelikler Seçiliyor...\")\n",
        "importance_threshold = 0.2\n",
        "selected_features = [item['feature'] for item in features_importance if item['importance'] >= importance_threshold]\n",
        "X = df_train[selected_features]\n",
        "print(f\"Önemi >= {importance_threshold} olan {len(selected_features)} adet özellik seçildi.\")\n",
        "\n",
        "# --- Veriyi Hazırlama ---\n",
        "y = df_train['session_value']\n",
        "y_log = np.log1p(y)\n",
        "\n",
        "# Aynı doğrulama setini kullanmak için ayırma işlemini tekrar yapıyoruz\n",
        "X_train, X_val, y_train_log, y_val_log = train_test_split(\n",
        "    X, y_log, test_size=0.2, shuffle=False\n",
        ")\n",
        "\n",
        "# Objective fonksiyonu (GÜNCELLENDİ)\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        'objective': 'RMSE',\n",
        "        'iterations': 4500,\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
        "        'depth': trial.suggest_int('depth', 4, 10),\n",
        "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10, log=True),\n",
        "        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 1.0),\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
        "        'random_seed': 42,\n",
        "        'verbose': 0,\n",
        "    }\n",
        "\n",
        "    model = CatBoostRegressor(**params)\n",
        "    # Modeli eğitirken hem train (X_train) hem de val (X_val) setini izlemesini sağlıyoruz\n",
        "    model.fit(\n",
        "        X_train, y_train_log,\n",
        "        eval_set=[(X_train, y_train_log), (X_val, y_val_log)], # İki seti de ekledik\n",
        "        early_stopping_rounds=250,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # En iyi iterasyondaki skorları al\n",
        "    scores = model.get_best_score()\n",
        "\n",
        "    best_learn_rmse = scores['learn']['RMSE']\n",
        "    val_rmse_0 = scores['validation_0']['RMSE']\n",
        "    val_rmse_1 = scores['validation_1']['RMSE']\n",
        "\n",
        "    # MSE'yi hesapla\n",
        "    preds_log = model.predict(X_val)\n",
        "    preds = np.expm1(preds_log)\n",
        "    preds[preds < 0] = 0\n",
        "    y_val_original = np.expm1(y_val_log)\n",
        "    mse = mean_squared_error(y_val_original, preds)\n",
        "\n",
        "    # --- GÜNCELLENMİŞ RAPORLAMA KISMI ---\n",
        "    print(f\"✅ Trial {trial.number} bitti | MSE: {mse:.4f} | Learn RMSE: {best_learn_rmse:.4f} | Val_0 RMSE: {val_rmse_0:.4f} | Val_1 RMSE : {val_rmse_1:.4f} | İterasyon: {model.get_best_iteration()}\")\n",
        "    gc.collect()\n",
        "    return mse\n",
        "\n",
        "\n",
        "# --- Optimizasyon Sürecini Başlatma (VERİTABANI İLE) ---\n",
        "# SQLite veritabanı için bağlantı dizesi oluştur\n",
        "storage_name = f\"sqlite:///{DB_FILENAME}\"\n",
        "\n",
        "# Çalışmayı oluştur veya varsa veritabanından yükle\n",
        "study = optuna.create_study(\n",
        "    study_name=STUDY_NAME,\n",
        "    storage=storage_name,\n",
        "    direction='minimize',\n",
        "    load_if_exists=True # Eğer bu isimde bir çalışma varsa, sıfırdan başlamak yerine devam et\n",
        ")\n",
        "\n",
        "# Optimizasyonu çalıştır\n",
        "n_trials = 100\n",
        "print(f\"Optimizasyon başlıyor... Sonuçlar '{DB_FILENAME}' dosyasına kaydedilecek.\")\n",
        "print(f\"Mevcut deneme sayısı: {len(study.trials)}. Toplamda {n_trials} denemeye ulaşılacak.\")\n",
        "study.optimize(objective, n_trials=n_trials)\n",
        "\n",
        "# --- Sonuçları Yazdırma ---\n",
        "print(\"\\nOptimizasyon Tamamlandı!\")\n",
        "print(f\"Toplam deneme sayısı: {len(study.trials)}\")\n",
        "print(f\"En iyi denemenin skoru (MSE): {study.best_value}\")\n",
        "print(\"En iyi denemenin parametreleri:\")\n",
        "for key, value in study.best_params.items():\n",
        "    print(f\"    {key}: {value}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.12.3)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
